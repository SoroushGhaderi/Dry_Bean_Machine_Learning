{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Dry_Bean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoroushGhaderi/Dry_Bean_Machine_Learning/blob/master/Dry_Bean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWtTxQKV1_Gp",
        "outputId": "9e27aa34-8db6-405d-bf76-15621ab07585"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlixUSTf1No9"
      },
      "source": [
        "import time\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WNQU04mT1NpD",
        "outputId": "d39d2b3a-1408-4da2-fccd-6923057a7299"
      },
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/Dry_Bean/data/Dry_Bean_Dataset.xlsx\"\n",
        "dry_beans = pd.read_excel(DATA_PATH)\n",
        "dry_beans.head()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Perimeter</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>AspectRation</th>\n",
              "      <th>Eccentricity</th>\n",
              "      <th>ConvexArea</th>\n",
              "      <th>EquivDiameter</th>\n",
              "      <th>Extent</th>\n",
              "      <th>Solidity</th>\n",
              "      <th>roundness</th>\n",
              "      <th>Compactness</th>\n",
              "      <th>ShapeFactor1</th>\n",
              "      <th>ShapeFactor2</th>\n",
              "      <th>ShapeFactor3</th>\n",
              "      <th>ShapeFactor4</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28395</td>\n",
              "      <td>610.291</td>\n",
              "      <td>208.178117</td>\n",
              "      <td>173.888747</td>\n",
              "      <td>1.197191</td>\n",
              "      <td>0.549812</td>\n",
              "      <td>28715</td>\n",
              "      <td>190.141097</td>\n",
              "      <td>0.763923</td>\n",
              "      <td>0.988856</td>\n",
              "      <td>0.958027</td>\n",
              "      <td>0.913358</td>\n",
              "      <td>0.007332</td>\n",
              "      <td>0.003147</td>\n",
              "      <td>0.834222</td>\n",
              "      <td>0.998724</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28734</td>\n",
              "      <td>638.018</td>\n",
              "      <td>200.524796</td>\n",
              "      <td>182.734419</td>\n",
              "      <td>1.097356</td>\n",
              "      <td>0.411785</td>\n",
              "      <td>29172</td>\n",
              "      <td>191.272750</td>\n",
              "      <td>0.783968</td>\n",
              "      <td>0.984986</td>\n",
              "      <td>0.887034</td>\n",
              "      <td>0.953861</td>\n",
              "      <td>0.006979</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>0.909851</td>\n",
              "      <td>0.998430</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29380</td>\n",
              "      <td>624.110</td>\n",
              "      <td>212.826130</td>\n",
              "      <td>175.931143</td>\n",
              "      <td>1.209713</td>\n",
              "      <td>0.562727</td>\n",
              "      <td>29690</td>\n",
              "      <td>193.410904</td>\n",
              "      <td>0.778113</td>\n",
              "      <td>0.989559</td>\n",
              "      <td>0.947849</td>\n",
              "      <td>0.908774</td>\n",
              "      <td>0.007244</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>0.825871</td>\n",
              "      <td>0.999066</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30008</td>\n",
              "      <td>645.884</td>\n",
              "      <td>210.557999</td>\n",
              "      <td>182.516516</td>\n",
              "      <td>1.153638</td>\n",
              "      <td>0.498616</td>\n",
              "      <td>30724</td>\n",
              "      <td>195.467062</td>\n",
              "      <td>0.782681</td>\n",
              "      <td>0.976696</td>\n",
              "      <td>0.903936</td>\n",
              "      <td>0.928329</td>\n",
              "      <td>0.007017</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.861794</td>\n",
              "      <td>0.994199</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30140</td>\n",
              "      <td>620.134</td>\n",
              "      <td>201.847882</td>\n",
              "      <td>190.279279</td>\n",
              "      <td>1.060798</td>\n",
              "      <td>0.333680</td>\n",
              "      <td>30417</td>\n",
              "      <td>195.896503</td>\n",
              "      <td>0.773098</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>0.984877</td>\n",
              "      <td>0.970516</td>\n",
              "      <td>0.006697</td>\n",
              "      <td>0.003665</td>\n",
              "      <td>0.941900</td>\n",
              "      <td>0.999166</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Area  Perimeter  MajorAxisLength  ...  ShapeFactor3  ShapeFactor4  Class\n",
              "0  28395    610.291       208.178117  ...      0.834222      0.998724  SEKER\n",
              "1  28734    638.018       200.524796  ...      0.909851      0.998430  SEKER\n",
              "2  29380    624.110       212.826130  ...      0.825871      0.999066  SEKER\n",
              "3  30008    645.884       210.557999  ...      0.861794      0.994199  SEKER\n",
              "4  30140    620.134       201.847882  ...      0.941900      0.999166  SEKER\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qru6CveK3TKq"
      },
      "source": [
        "class DryBeanDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = pd.read_excel(path)\n",
        "        self.X = torch.Tensor(np.array(data.iloc[:, :-1], dtype=np.float32))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.y = torch.Tensor(self.label_encoder.fit_transform(data.iloc[:, -1]))\n",
        "        self.y_label_classes = self.label_encoder.classes_\n",
        "        self.n_samples = data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return self.X[item], self.y[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "    def inverse_encoder(self):\n",
        "        return self.y_label_classes"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofbrAc0bEQBG"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(16,50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(50, 7)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = self.relu1(output)\n",
        "        output = self.fc2(output)\n",
        "        return output"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrahFDUj5_53"
      },
      "source": [
        "dry_bean = DryBeanDataset(DATA_PATH)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKwuXtAkNN3"
      },
      "source": [
        "X, y = dry_bean[:]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of1FtD4f6_sC"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofT0a8JNtbf-",
        "outputId": "a681db89-9f7e-4f3b-9fe6-c21f50d7e43e"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNF2Iu8Atgz3",
        "outputId": "317807f1-764d-481c-8c5c-6bea79ba3e22"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7OUAwWWEEy_"
      },
      "source": [
        "train_data = TensorDataset(X_train, y_train.type(torch.LongTensor))\n",
        "test_data = TensorDataset(X_test, y_test.type(torch.LongTensor))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vylABOby1Utl"
      },
      "source": [
        ""
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AZjfv8Dr1Lc"
      },
      "source": [
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epoch = 500"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb6MOgDGn-p5"
      },
      "source": [
        "network = MLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = torch.optim.Adam(network.parameters(), lr=learning_rate)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0wxMAlmoghA"
      },
      "source": [
        "data = {\"train\": train_data, \"val\": test_data}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(data[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(data[x]) for x in ['train', 'val']}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmhZk5f937YD"
      },
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rr_sg6NoIzv",
        "outputId": "a54b9f0f-26eb-4965-b59b-8e01632456e1"
      },
      "source": [
        "model_ft = train_model(network, criterion, optimizer_ft, num_epochs=num_epoch)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/499\n",
            "----------\n",
            "train Loss: 842.0302 Acc: 0.1260\n",
            "val Loss: 44.3088 Acc: 0.1293\n",
            "\n",
            "Epoch 1/499\n",
            "----------\n",
            "train Loss: 43.0986 Acc: 0.1454\n",
            "val Loss: 35.4811 Acc: 0.1689\n",
            "\n",
            "Epoch 2/499\n",
            "----------\n",
            "train Loss: 47.8105 Acc: 0.1690\n",
            "val Loss: 23.8334 Acc: 0.1271\n",
            "\n",
            "Epoch 3/499\n",
            "----------\n",
            "train Loss: 54.6357 Acc: 0.1739\n",
            "val Loss: 50.4286 Acc: 0.0547\n",
            "\n",
            "Epoch 4/499\n",
            "----------\n",
            "train Loss: 49.0022 Acc: 0.2001\n",
            "val Loss: 38.2499 Acc: 0.4003\n",
            "\n",
            "Epoch 5/499\n",
            "----------\n",
            "train Loss: 38.4766 Acc: 0.2211\n",
            "val Loss: 50.9986 Acc: 0.0437\n",
            "\n",
            "Epoch 6/499\n",
            "----------\n",
            "train Loss: 36.0820 Acc: 0.2370\n",
            "val Loss: 17.2661 Acc: 0.4106\n",
            "\n",
            "Epoch 7/499\n",
            "----------\n",
            "train Loss: 27.7362 Acc: 0.2559\n",
            "val Loss: 21.4742 Acc: 0.2989\n",
            "\n",
            "Epoch 8/499\n",
            "----------\n",
            "train Loss: 31.5897 Acc: 0.2452\n",
            "val Loss: 26.9083 Acc: 0.3489\n",
            "\n",
            "Epoch 9/499\n",
            "----------\n",
            "train Loss: 26.9966 Acc: 0.2924\n",
            "val Loss: 22.1685 Acc: 0.1480\n",
            "\n",
            "Epoch 10/499\n",
            "----------\n",
            "train Loss: 37.4693 Acc: 0.2772\n",
            "val Loss: 42.4018 Acc: 0.2497\n",
            "\n",
            "Epoch 11/499\n",
            "----------\n",
            "train Loss: 26.6415 Acc: 0.3236\n",
            "val Loss: 23.5821 Acc: 0.3562\n",
            "\n",
            "Epoch 12/499\n",
            "----------\n",
            "train Loss: 20.5757 Acc: 0.3509\n",
            "val Loss: 24.1449 Acc: 0.2402\n",
            "\n",
            "Epoch 13/499\n",
            "----------\n",
            "train Loss: 28.7290 Acc: 0.3163\n",
            "val Loss: 28.5400 Acc: 0.1620\n",
            "\n",
            "Epoch 14/499\n",
            "----------\n",
            "train Loss: 29.8203 Acc: 0.2967\n",
            "val Loss: 46.8823 Acc: 0.2141\n",
            "\n",
            "Epoch 15/499\n",
            "----------\n",
            "train Loss: 17.8887 Acc: 0.3474\n",
            "val Loss: 21.4130 Acc: 0.4425\n",
            "\n",
            "Epoch 16/499\n",
            "----------\n",
            "train Loss: 17.0767 Acc: 0.3839\n",
            "val Loss: 31.3296 Acc: 0.3261\n",
            "\n",
            "Epoch 17/499\n",
            "----------\n",
            "train Loss: 20.5664 Acc: 0.3591\n",
            "val Loss: 16.1062 Acc: 0.2806\n",
            "\n",
            "Epoch 18/499\n",
            "----------\n",
            "train Loss: 22.1021 Acc: 0.3508\n",
            "val Loss: 17.8062 Acc: 0.4348\n",
            "\n",
            "Epoch 19/499\n",
            "----------\n",
            "train Loss: 15.7802 Acc: 0.3804\n",
            "val Loss: 12.4323 Acc: 0.4943\n",
            "\n",
            "Epoch 20/499\n",
            "----------\n",
            "train Loss: 16.8422 Acc: 0.4042\n",
            "val Loss: 18.7117 Acc: 0.2754\n",
            "\n",
            "Epoch 21/499\n",
            "----------\n",
            "train Loss: 13.8243 Acc: 0.4140\n",
            "val Loss: 6.4868 Acc: 0.5476\n",
            "\n",
            "Epoch 22/499\n",
            "----------\n",
            "train Loss: 15.4076 Acc: 0.4011\n",
            "val Loss: 29.4830 Acc: 0.3070\n",
            "\n",
            "Epoch 23/499\n",
            "----------\n",
            "train Loss: 21.9257 Acc: 0.3524\n",
            "val Loss: 21.3920 Acc: 0.1465\n",
            "\n",
            "Epoch 24/499\n",
            "----------\n",
            "train Loss: 15.8570 Acc: 0.4044\n",
            "val Loss: 23.9971 Acc: 0.4447\n",
            "\n",
            "Epoch 25/499\n",
            "----------\n",
            "train Loss: 19.4398 Acc: 0.3772\n",
            "val Loss: 33.0052 Acc: 0.3026\n",
            "\n",
            "Epoch 26/499\n",
            "----------\n",
            "train Loss: 17.9023 Acc: 0.3970\n",
            "val Loss: 23.7254 Acc: 0.2699\n",
            "\n",
            "Epoch 27/499\n",
            "----------\n",
            "train Loss: 13.4887 Acc: 0.4192\n",
            "val Loss: 14.0161 Acc: 0.4098\n",
            "\n",
            "Epoch 28/499\n",
            "----------\n",
            "train Loss: 14.0609 Acc: 0.4202\n",
            "val Loss: 23.6058 Acc: 0.3353\n",
            "\n",
            "Epoch 29/499\n",
            "----------\n",
            "train Loss: 14.4980 Acc: 0.4211\n",
            "val Loss: 13.6772 Acc: 0.3511\n",
            "\n",
            "Epoch 30/499\n",
            "----------\n",
            "train Loss: 13.7750 Acc: 0.4233\n",
            "val Loss: 17.4243 Acc: 0.5053\n",
            "\n",
            "Epoch 31/499\n",
            "----------\n",
            "train Loss: 11.1724 Acc: 0.4582\n",
            "val Loss: 14.5041 Acc: 0.4594\n",
            "\n",
            "Epoch 32/499\n",
            "----------\n",
            "train Loss: 9.6841 Acc: 0.4810\n",
            "val Loss: 13.3466 Acc: 0.4557\n",
            "\n",
            "Epoch 33/499\n",
            "----------\n",
            "train Loss: 16.8480 Acc: 0.4172\n",
            "val Loss: 16.2746 Acc: 0.3577\n",
            "\n",
            "Epoch 34/499\n",
            "----------\n",
            "train Loss: 14.9033 Acc: 0.4172\n",
            "val Loss: 13.3275 Acc: 0.3907\n",
            "\n",
            "Epoch 35/499\n",
            "----------\n",
            "train Loss: 12.7271 Acc: 0.4445\n",
            "val Loss: 12.3182 Acc: 0.5200\n",
            "\n",
            "Epoch 36/499\n",
            "----------\n",
            "train Loss: 7.8829 Acc: 0.5168\n",
            "val Loss: 5.7449 Acc: 0.6515\n",
            "\n",
            "Epoch 37/499\n",
            "----------\n",
            "train Loss: 11.4361 Acc: 0.4668\n",
            "val Loss: 15.5613 Acc: 0.3235\n",
            "\n",
            "Epoch 38/499\n",
            "----------\n",
            "train Loss: 14.2762 Acc: 0.4184\n",
            "val Loss: 14.5512 Acc: 0.4150\n",
            "\n",
            "Epoch 39/499\n",
            "----------\n",
            "train Loss: 13.9226 Acc: 0.4387\n",
            "val Loss: 12.0668 Acc: 0.4095\n",
            "\n",
            "Epoch 40/499\n",
            "----------\n",
            "train Loss: 13.5524 Acc: 0.4354\n",
            "val Loss: 21.8591 Acc: 0.3089\n",
            "\n",
            "Epoch 41/499\n",
            "----------\n",
            "train Loss: 14.9773 Acc: 0.4533\n",
            "val Loss: 12.9786 Acc: 0.4032\n",
            "\n",
            "Epoch 42/499\n",
            "----------\n",
            "train Loss: 8.4802 Acc: 0.4996\n",
            "val Loss: 28.3125 Acc: 0.4429\n",
            "\n",
            "Epoch 43/499\n",
            "----------\n",
            "train Loss: 9.1405 Acc: 0.5079\n",
            "val Loss: 13.5772 Acc: 0.4451\n",
            "\n",
            "Epoch 44/499\n",
            "----------\n",
            "train Loss: 10.7618 Acc: 0.4726\n",
            "val Loss: 11.1249 Acc: 0.4095\n",
            "\n",
            "Epoch 45/499\n",
            "----------\n",
            "train Loss: 11.5790 Acc: 0.4835\n",
            "val Loss: 27.9006 Acc: 0.3202\n",
            "\n",
            "Epoch 46/499\n",
            "----------\n",
            "train Loss: 19.6085 Acc: 0.4104\n",
            "val Loss: 26.8738 Acc: 0.3415\n",
            "\n",
            "Epoch 47/499\n",
            "----------\n",
            "train Loss: 15.1587 Acc: 0.4807\n",
            "val Loss: 25.6888 Acc: 0.2953\n",
            "\n",
            "Epoch 48/499\n",
            "----------\n",
            "train Loss: 6.6747 Acc: 0.5501\n",
            "val Loss: 3.8806 Acc: 0.5957\n",
            "\n",
            "Epoch 49/499\n",
            "----------\n",
            "train Loss: 6.1929 Acc: 0.5550\n",
            "val Loss: 14.1077 Acc: 0.4275\n",
            "\n",
            "Epoch 50/499\n",
            "----------\n",
            "train Loss: 7.6208 Acc: 0.5454\n",
            "val Loss: 8.9849 Acc: 0.5443\n",
            "\n",
            "Epoch 51/499\n",
            "----------\n",
            "train Loss: 5.4276 Acc: 0.5849\n",
            "val Loss: 17.0769 Acc: 0.4087\n",
            "\n",
            "Epoch 52/499\n",
            "----------\n",
            "train Loss: 10.1394 Acc: 0.5162\n",
            "val Loss: 11.1170 Acc: 0.5626\n",
            "\n",
            "Epoch 53/499\n",
            "----------\n",
            "train Loss: 13.0181 Acc: 0.4922\n",
            "val Loss: 21.1472 Acc: 0.3430\n",
            "\n",
            "Epoch 54/499\n",
            "----------\n",
            "train Loss: 14.5293 Acc: 0.4581\n",
            "val Loss: 8.4085 Acc: 0.4910\n",
            "\n",
            "Epoch 55/499\n",
            "----------\n",
            "train Loss: 7.1423 Acc: 0.5691\n",
            "val Loss: 10.7478 Acc: 0.4025\n",
            "\n",
            "Epoch 56/499\n",
            "----------\n",
            "train Loss: 9.3608 Acc: 0.5341\n",
            "val Loss: 4.0482 Acc: 0.5894\n",
            "\n",
            "Epoch 57/499\n",
            "----------\n",
            "train Loss: 4.2401 Acc: 0.6334\n",
            "val Loss: 13.3189 Acc: 0.5340\n",
            "\n",
            "Epoch 58/499\n",
            "----------\n",
            "train Loss: 8.3277 Acc: 0.5338\n",
            "val Loss: 18.4521 Acc: 0.5002\n",
            "\n",
            "Epoch 59/499\n",
            "----------\n",
            "train Loss: 16.5578 Acc: 0.4640\n",
            "val Loss: 17.8730 Acc: 0.3797\n",
            "\n",
            "Epoch 60/499\n",
            "----------\n",
            "train Loss: 18.8193 Acc: 0.4291\n",
            "val Loss: 6.2476 Acc: 0.5784\n",
            "\n",
            "Epoch 61/499\n",
            "----------\n",
            "train Loss: 9.2361 Acc: 0.5357\n",
            "val Loss: 30.2758 Acc: 0.2284\n",
            "\n",
            "Epoch 62/499\n",
            "----------\n",
            "train Loss: 10.5548 Acc: 0.5186\n",
            "val Loss: 5.9918 Acc: 0.4958\n",
            "\n",
            "Epoch 63/499\n",
            "----------\n",
            "train Loss: 11.9165 Acc: 0.4845\n",
            "val Loss: 18.1643 Acc: 0.4054\n",
            "\n",
            "Epoch 64/499\n",
            "----------\n",
            "train Loss: 7.0114 Acc: 0.5743\n",
            "val Loss: 5.8124 Acc: 0.5138\n",
            "\n",
            "Epoch 65/499\n",
            "----------\n",
            "train Loss: 10.0255 Acc: 0.5223\n",
            "val Loss: 26.9901 Acc: 0.2604\n",
            "\n",
            "Epoch 66/499\n",
            "----------\n",
            "train Loss: 14.0728 Acc: 0.5112\n",
            "val Loss: 7.9628 Acc: 0.5942\n",
            "\n",
            "Epoch 67/499\n",
            "----------\n",
            "train Loss: 6.3593 Acc: 0.5843\n",
            "val Loss: 9.6134 Acc: 0.5982\n",
            "\n",
            "Epoch 68/499\n",
            "----------\n",
            "train Loss: 4.2097 Acc: 0.6413\n",
            "val Loss: 4.7025 Acc: 0.6331\n",
            "\n",
            "Epoch 69/499\n",
            "----------\n",
            "train Loss: 7.5623 Acc: 0.5761\n",
            "val Loss: 10.3970 Acc: 0.6159\n",
            "\n",
            "Epoch 70/499\n",
            "----------\n",
            "train Loss: 10.9827 Acc: 0.5301\n",
            "val Loss: 12.2073 Acc: 0.5835\n",
            "\n",
            "Epoch 71/499\n",
            "----------\n",
            "train Loss: 7.2945 Acc: 0.5773\n",
            "val Loss: 16.9787 Acc: 0.5795\n",
            "\n",
            "Epoch 72/499\n",
            "----------\n",
            "train Loss: 5.6668 Acc: 0.6262\n",
            "val Loss: 3.6133 Acc: 0.6023\n",
            "\n",
            "Epoch 73/499\n",
            "----------\n",
            "train Loss: 2.8462 Acc: 0.6836\n",
            "val Loss: 7.7395 Acc: 0.6074\n",
            "\n",
            "Epoch 74/499\n",
            "----------\n",
            "train Loss: 6.4468 Acc: 0.5904\n",
            "val Loss: 4.6683 Acc: 0.5769\n",
            "\n",
            "Epoch 75/499\n",
            "----------\n",
            "train Loss: 6.2286 Acc: 0.5866\n",
            "val Loss: 7.2809 Acc: 0.5211\n",
            "\n",
            "Epoch 76/499\n",
            "----------\n",
            "train Loss: 7.9835 Acc: 0.5681\n",
            "val Loss: 2.3308 Acc: 0.6849\n",
            "\n",
            "Epoch 77/499\n",
            "----------\n",
            "train Loss: 7.3136 Acc: 0.5766\n",
            "val Loss: 9.7080 Acc: 0.4216\n",
            "\n",
            "Epoch 78/499\n",
            "----------\n",
            "train Loss: 5.4762 Acc: 0.6057\n",
            "val Loss: 4.4745 Acc: 0.5751\n",
            "\n",
            "Epoch 79/499\n",
            "----------\n",
            "train Loss: 10.9943 Acc: 0.5432\n",
            "val Loss: 16.0235 Acc: 0.6287\n",
            "\n",
            "Epoch 80/499\n",
            "----------\n",
            "train Loss: 7.3347 Acc: 0.6023\n",
            "val Loss: 20.6672 Acc: 0.3647\n",
            "\n",
            "Epoch 81/499\n",
            "----------\n",
            "train Loss: 11.1514 Acc: 0.5354\n",
            "val Loss: 4.9410 Acc: 0.6772\n",
            "\n",
            "Epoch 82/499\n",
            "----------\n",
            "train Loss: 3.7232 Acc: 0.6806\n",
            "val Loss: 6.1888 Acc: 0.5824\n",
            "\n",
            "Epoch 83/499\n",
            "----------\n",
            "train Loss: 5.7395 Acc: 0.6097\n",
            "val Loss: 13.5045 Acc: 0.3996\n",
            "\n",
            "Epoch 84/499\n",
            "----------\n",
            "train Loss: 4.6775 Acc: 0.6572\n",
            "val Loss: 7.4793 Acc: 0.5960\n",
            "\n",
            "Epoch 85/499\n",
            "----------\n",
            "train Loss: 6.5309 Acc: 0.6081\n",
            "val Loss: 13.9340 Acc: 0.5237\n",
            "\n",
            "Epoch 86/499\n",
            "----------\n",
            "train Loss: 10.0168 Acc: 0.5482\n",
            "val Loss: 20.7036 Acc: 0.3180\n",
            "\n",
            "Epoch 87/499\n",
            "----------\n",
            "train Loss: 6.4247 Acc: 0.6084\n",
            "val Loss: 12.9901 Acc: 0.4793\n",
            "\n",
            "Epoch 88/499\n",
            "----------\n",
            "train Loss: 4.8068 Acc: 0.6620\n",
            "val Loss: 5.3252 Acc: 0.7304\n",
            "\n",
            "Epoch 89/499\n",
            "----------\n",
            "train Loss: 3.2515 Acc: 0.7211\n",
            "val Loss: 4.7642 Acc: 0.6640\n",
            "\n",
            "Epoch 90/499\n",
            "----------\n",
            "train Loss: 4.8119 Acc: 0.6546\n",
            "val Loss: 9.3406 Acc: 0.4293\n",
            "\n",
            "Epoch 91/499\n",
            "----------\n",
            "train Loss: 11.1570 Acc: 0.5336\n",
            "val Loss: 18.2651 Acc: 0.3614\n",
            "\n",
            "Epoch 92/499\n",
            "----------\n",
            "train Loss: 8.5619 Acc: 0.5710\n",
            "val Loss: 3.8446 Acc: 0.6383\n",
            "\n",
            "Epoch 93/499\n",
            "----------\n",
            "train Loss: 4.2786 Acc: 0.6585\n",
            "val Loss: 9.3685 Acc: 0.6757\n",
            "\n",
            "Epoch 94/499\n",
            "----------\n",
            "train Loss: 2.8922 Acc: 0.7271\n",
            "val Loss: 19.0785 Acc: 0.5630\n",
            "\n",
            "Epoch 95/499\n",
            "----------\n",
            "train Loss: 8.6895 Acc: 0.5861\n",
            "val Loss: 7.4041 Acc: 0.6306\n",
            "\n",
            "Epoch 96/499\n",
            "----------\n",
            "train Loss: 4.4882 Acc: 0.6650\n",
            "val Loss: 4.1593 Acc: 0.7360\n",
            "\n",
            "Epoch 97/499\n",
            "----------\n",
            "train Loss: 7.9882 Acc: 0.5974\n",
            "val Loss: 11.4422 Acc: 0.4139\n",
            "\n",
            "Epoch 98/499\n",
            "----------\n",
            "train Loss: 4.5411 Acc: 0.6601\n",
            "val Loss: 10.8121 Acc: 0.5450\n",
            "\n",
            "Epoch 99/499\n",
            "----------\n",
            "train Loss: 5.4556 Acc: 0.6431\n",
            "val Loss: 6.0338 Acc: 0.6515\n",
            "\n",
            "Epoch 100/499\n",
            "----------\n",
            "train Loss: 3.9852 Acc: 0.6810\n",
            "val Loss: 8.9199 Acc: 0.6166\n",
            "\n",
            "Epoch 101/499\n",
            "----------\n",
            "train Loss: 2.8957 Acc: 0.7349\n",
            "val Loss: 2.7427 Acc: 0.7499\n",
            "\n",
            "Epoch 102/499\n",
            "----------\n",
            "train Loss: 3.9080 Acc: 0.6948\n",
            "val Loss: 12.0138 Acc: 0.4936\n",
            "\n",
            "Epoch 103/499\n",
            "----------\n",
            "train Loss: 5.7068 Acc: 0.6429\n",
            "val Loss: 18.7261 Acc: 0.5909\n",
            "\n",
            "Epoch 104/499\n",
            "----------\n",
            "train Loss: 9.7063 Acc: 0.5755\n",
            "val Loss: 10.1998 Acc: 0.6563\n",
            "\n",
            "Epoch 105/499\n",
            "----------\n",
            "train Loss: 3.1775 Acc: 0.7272\n",
            "val Loss: 4.6124 Acc: 0.7139\n",
            "\n",
            "Epoch 106/499\n",
            "----------\n",
            "train Loss: 5.2897 Acc: 0.6492\n",
            "val Loss: 8.8781 Acc: 0.5813\n",
            "\n",
            "Epoch 107/499\n",
            "----------\n",
            "train Loss: 5.4961 Acc: 0.6537\n",
            "val Loss: 9.0394 Acc: 0.5310\n",
            "\n",
            "Epoch 108/499\n",
            "----------\n",
            "train Loss: 8.7155 Acc: 0.5895\n",
            "val Loss: 9.0426 Acc: 0.6140\n",
            "\n",
            "Epoch 109/499\n",
            "----------\n",
            "train Loss: 3.3442 Acc: 0.7222\n",
            "val Loss: 9.7049 Acc: 0.5527\n",
            "\n",
            "Epoch 110/499\n",
            "----------\n",
            "train Loss: 5.0453 Acc: 0.6653\n",
            "val Loss: 3.5005 Acc: 0.7172\n",
            "\n",
            "Epoch 111/499\n",
            "----------\n",
            "train Loss: 2.9590 Acc: 0.7258\n",
            "val Loss: 8.7460 Acc: 0.6111\n",
            "\n",
            "Epoch 112/499\n",
            "----------\n",
            "train Loss: 4.5066 Acc: 0.6814\n",
            "val Loss: 4.7466 Acc: 0.6078\n",
            "\n",
            "Epoch 113/499\n",
            "----------\n",
            "train Loss: 2.5394 Acc: 0.7552\n",
            "val Loss: 10.6052 Acc: 0.5446\n",
            "\n",
            "Epoch 114/499\n",
            "----------\n",
            "train Loss: 3.9449 Acc: 0.7095\n",
            "val Loss: 4.2026 Acc: 0.6456\n",
            "\n",
            "Epoch 115/499\n",
            "----------\n",
            "train Loss: 6.5949 Acc: 0.6502\n",
            "val Loss: 11.9988 Acc: 0.4928\n",
            "\n",
            "Epoch 116/499\n",
            "----------\n",
            "train Loss: 7.0040 Acc: 0.6447\n",
            "val Loss: 8.3477 Acc: 0.6236\n",
            "\n",
            "Epoch 117/499\n",
            "----------\n",
            "train Loss: 7.1579 Acc: 0.6266\n",
            "val Loss: 12.1657 Acc: 0.4910\n",
            "\n",
            "Epoch 118/499\n",
            "----------\n",
            "train Loss: 3.8941 Acc: 0.6985\n",
            "val Loss: 15.6235 Acc: 0.4300\n",
            "\n",
            "Epoch 119/499\n",
            "----------\n",
            "train Loss: 5.0216 Acc: 0.6870\n",
            "val Loss: 2.7120 Acc: 0.8006\n",
            "\n",
            "Epoch 120/499\n",
            "----------\n",
            "train Loss: 2.0894 Acc: 0.7765\n",
            "val Loss: 8.4311 Acc: 0.5329\n",
            "\n",
            "Epoch 121/499\n",
            "----------\n",
            "train Loss: 5.9169 Acc: 0.6465\n",
            "val Loss: 12.4306 Acc: 0.6592\n",
            "\n",
            "Epoch 122/499\n",
            "----------\n",
            "train Loss: 6.6150 Acc: 0.6503\n",
            "val Loss: 6.3724 Acc: 0.6107\n",
            "\n",
            "Epoch 123/499\n",
            "----------\n",
            "train Loss: 5.0902 Acc: 0.6870\n",
            "val Loss: 6.0911 Acc: 0.6416\n",
            "\n",
            "Epoch 124/499\n",
            "----------\n",
            "train Loss: 3.3023 Acc: 0.7398\n",
            "val Loss: 6.3662 Acc: 0.5744\n",
            "\n",
            "Epoch 125/499\n",
            "----------\n",
            "train Loss: 2.5941 Acc: 0.7582\n",
            "val Loss: 2.0842 Acc: 0.7606\n",
            "\n",
            "Epoch 126/499\n",
            "----------\n",
            "train Loss: 5.1879 Acc: 0.6879\n",
            "val Loss: 6.8957 Acc: 0.6996\n",
            "\n",
            "Epoch 127/499\n",
            "----------\n",
            "train Loss: 5.7551 Acc: 0.6589\n",
            "val Loss: 11.8638 Acc: 0.5854\n",
            "\n",
            "Epoch 128/499\n",
            "----------\n",
            "train Loss: 4.0584 Acc: 0.7238\n",
            "val Loss: 5.7017 Acc: 0.6588\n",
            "\n",
            "Epoch 129/499\n",
            "----------\n",
            "train Loss: 3.7274 Acc: 0.7152\n",
            "val Loss: 10.3260 Acc: 0.6056\n",
            "\n",
            "Epoch 130/499\n",
            "----------\n",
            "train Loss: 3.5350 Acc: 0.7481\n",
            "val Loss: 4.1303 Acc: 0.6882\n",
            "\n",
            "Epoch 131/499\n",
            "----------\n",
            "train Loss: 2.2029 Acc: 0.7884\n",
            "val Loss: 4.9496 Acc: 0.7268\n",
            "\n",
            "Epoch 132/499\n",
            "----------\n",
            "train Loss: 3.0654 Acc: 0.7311\n",
            "val Loss: 10.7699 Acc: 0.5545\n",
            "\n",
            "Epoch 133/499\n",
            "----------\n",
            "train Loss: 5.1718 Acc: 0.6976\n",
            "val Loss: 16.2831 Acc: 0.5465\n",
            "\n",
            "Epoch 134/499\n",
            "----------\n",
            "train Loss: 8.7419 Acc: 0.6309\n",
            "val Loss: 10.3690 Acc: 0.5226\n",
            "\n",
            "Epoch 135/499\n",
            "----------\n",
            "train Loss: 3.4966 Acc: 0.7363\n",
            "val Loss: 2.5960 Acc: 0.7455\n",
            "\n",
            "Epoch 136/499\n",
            "----------\n",
            "train Loss: 3.4253 Acc: 0.7571\n",
            "val Loss: 2.2184 Acc: 0.8002\n",
            "\n",
            "Epoch 137/499\n",
            "----------\n",
            "train Loss: 5.8948 Acc: 0.6614\n",
            "val Loss: 12.7220 Acc: 0.5758\n",
            "\n",
            "Epoch 138/499\n",
            "----------\n",
            "train Loss: 3.5923 Acc: 0.7404\n",
            "val Loss: 3.8247 Acc: 0.6721\n",
            "\n",
            "Epoch 139/499\n",
            "----------\n",
            "train Loss: 4.6742 Acc: 0.7133\n",
            "val Loss: 20.9297 Acc: 0.4322\n",
            "\n",
            "Epoch 140/499\n",
            "----------\n",
            "train Loss: 8.3859 Acc: 0.6200\n",
            "val Loss: 6.6978 Acc: 0.6004\n",
            "\n",
            "Epoch 141/499\n",
            "----------\n",
            "train Loss: 2.5128 Acc: 0.7884\n",
            "val Loss: 18.7506 Acc: 0.4216\n",
            "\n",
            "Epoch 142/499\n",
            "----------\n",
            "train Loss: 3.9772 Acc: 0.7402\n",
            "val Loss: 6.6334 Acc: 0.6563\n",
            "\n",
            "Epoch 143/499\n",
            "----------\n",
            "train Loss: 5.3281 Acc: 0.6806\n",
            "val Loss: 5.0617 Acc: 0.6956\n",
            "\n",
            "Epoch 144/499\n",
            "----------\n",
            "train Loss: 4.7713 Acc: 0.6886\n",
            "val Loss: 3.0817 Acc: 0.7778\n",
            "\n",
            "Epoch 145/499\n",
            "----------\n",
            "train Loss: 2.2097 Acc: 0.8006\n",
            "val Loss: 4.9873 Acc: 0.6056\n",
            "\n",
            "Epoch 146/499\n",
            "----------\n",
            "train Loss: 3.2902 Acc: 0.7454\n",
            "val Loss: 2.8321 Acc: 0.7819\n",
            "\n",
            "Epoch 147/499\n",
            "----------\n",
            "train Loss: 3.7183 Acc: 0.7431\n",
            "val Loss: 5.7294 Acc: 0.6541\n",
            "\n",
            "Epoch 148/499\n",
            "----------\n",
            "train Loss: 5.1136 Acc: 0.7075\n",
            "val Loss: 4.9069 Acc: 0.7202\n",
            "\n",
            "Epoch 149/499\n",
            "----------\n",
            "train Loss: 4.0511 Acc: 0.7409\n",
            "val Loss: 27.9161 Acc: 0.3654\n",
            "\n",
            "Epoch 150/499\n",
            "----------\n",
            "train Loss: 5.7305 Acc: 0.7018\n",
            "val Loss: 10.5079 Acc: 0.4961\n",
            "\n",
            "Epoch 151/499\n",
            "----------\n",
            "train Loss: 3.3294 Acc: 0.7511\n",
            "val Loss: 12.1935 Acc: 0.5288\n",
            "\n",
            "Epoch 152/499\n",
            "----------\n",
            "train Loss: 4.7655 Acc: 0.7136\n",
            "val Loss: 7.6573 Acc: 0.6754\n",
            "\n",
            "Epoch 153/499\n",
            "----------\n",
            "train Loss: 5.2748 Acc: 0.6940\n",
            "val Loss: 4.9877 Acc: 0.6603\n",
            "\n",
            "Epoch 154/499\n",
            "----------\n",
            "train Loss: 3.1924 Acc: 0.7524\n",
            "val Loss: 2.9203 Acc: 0.7161\n",
            "\n",
            "Epoch 155/499\n",
            "----------\n",
            "train Loss: 2.5329 Acc: 0.7883\n",
            "val Loss: 5.0839 Acc: 0.6989\n",
            "\n",
            "Epoch 156/499\n",
            "----------\n",
            "train Loss: 4.6498 Acc: 0.7269\n",
            "val Loss: 4.0230 Acc: 0.7484\n",
            "\n",
            "Epoch 157/499\n",
            "----------\n",
            "train Loss: 3.4629 Acc: 0.7466\n",
            "val Loss: 16.0217 Acc: 0.4355\n",
            "\n",
            "Epoch 158/499\n",
            "----------\n",
            "train Loss: 5.1174 Acc: 0.7124\n",
            "val Loss: 3.3110 Acc: 0.7477\n",
            "\n",
            "Epoch 159/499\n",
            "----------\n",
            "train Loss: 2.4180 Acc: 0.7932\n",
            "val Loss: 1.2806 Acc: 0.8516\n",
            "\n",
            "Epoch 160/499\n",
            "----------\n",
            "train Loss: 2.8809 Acc: 0.7662\n",
            "val Loss: 12.8818 Acc: 0.5398\n",
            "\n",
            "Epoch 161/499\n",
            "----------\n",
            "train Loss: 6.9774 Acc: 0.6595\n",
            "val Loss: 21.7474 Acc: 0.3830\n",
            "\n",
            "Epoch 162/499\n",
            "----------\n",
            "train Loss: 6.2207 Acc: 0.7008\n",
            "val Loss: 5.5335 Acc: 0.6541\n",
            "\n",
            "Epoch 163/499\n",
            "----------\n",
            "train Loss: 3.7236 Acc: 0.7505\n",
            "val Loss: 13.0602 Acc: 0.6144\n",
            "\n",
            "Epoch 164/499\n",
            "----------\n",
            "train Loss: 6.3668 Acc: 0.6961\n",
            "val Loss: 1.6285 Acc: 0.8399\n",
            "\n",
            "Epoch 165/499\n",
            "----------\n",
            "train Loss: 3.1228 Acc: 0.7792\n",
            "val Loss: 9.9013 Acc: 0.5479\n",
            "\n",
            "Epoch 166/499\n",
            "----------\n",
            "train Loss: 3.1550 Acc: 0.7898\n",
            "val Loss: 3.9223 Acc: 0.7455\n",
            "\n",
            "Epoch 167/499\n",
            "----------\n",
            "train Loss: 6.4879 Acc: 0.7038\n",
            "val Loss: 16.0945 Acc: 0.4539\n",
            "\n",
            "Epoch 168/499\n",
            "----------\n",
            "train Loss: 4.5049 Acc: 0.7259\n",
            "val Loss: 4.8412 Acc: 0.7051\n",
            "\n",
            "Epoch 169/499\n",
            "----------\n",
            "train Loss: 2.3878 Acc: 0.8058\n",
            "val Loss: 4.6742 Acc: 0.6559\n",
            "\n",
            "Epoch 170/499\n",
            "----------\n",
            "train Loss: 3.5574 Acc: 0.7654\n",
            "val Loss: 16.3863 Acc: 0.4293\n",
            "\n",
            "Epoch 171/499\n",
            "----------\n",
            "train Loss: 4.8448 Acc: 0.7369\n",
            "val Loss: 1.8876 Acc: 0.8120\n",
            "\n",
            "Epoch 172/499\n",
            "----------\n",
            "train Loss: 5.4680 Acc: 0.6950\n",
            "val Loss: 8.3030 Acc: 0.6056\n",
            "\n",
            "Epoch 173/499\n",
            "----------\n",
            "train Loss: 3.8815 Acc: 0.7651\n",
            "val Loss: 3.7135 Acc: 0.7058\n",
            "\n",
            "Epoch 174/499\n",
            "----------\n",
            "train Loss: 3.6318 Acc: 0.7504\n",
            "val Loss: 9.0966 Acc: 0.6059\n",
            "\n",
            "Epoch 175/499\n",
            "----------\n",
            "train Loss: 4.1637 Acc: 0.7500\n",
            "val Loss: 5.1800 Acc: 0.7958\n",
            "\n",
            "Epoch 176/499\n",
            "----------\n",
            "train Loss: 3.3188 Acc: 0.7765\n",
            "val Loss: 7.7648 Acc: 0.6184\n",
            "\n",
            "Epoch 177/499\n",
            "----------\n",
            "train Loss: 3.4583 Acc: 0.7577\n",
            "val Loss: 4.2213 Acc: 0.7528\n",
            "\n",
            "Epoch 178/499\n",
            "----------\n",
            "train Loss: 4.4783 Acc: 0.7473\n",
            "val Loss: 11.3575 Acc: 0.4693\n",
            "\n",
            "Epoch 179/499\n",
            "----------\n",
            "train Loss: 2.6869 Acc: 0.8020\n",
            "val Loss: 5.5108 Acc: 0.6581\n",
            "\n",
            "Epoch 180/499\n",
            "----------\n",
            "train Loss: 2.8947 Acc: 0.7977\n",
            "val Loss: 3.6650 Acc: 0.7885\n",
            "\n",
            "Epoch 181/499\n",
            "----------\n",
            "train Loss: 3.3862 Acc: 0.7618\n",
            "val Loss: 5.0990 Acc: 0.6878\n",
            "\n",
            "Epoch 182/499\n",
            "----------\n",
            "train Loss: 2.5963 Acc: 0.7922\n",
            "val Loss: 7.9898 Acc: 0.6265\n",
            "\n",
            "Epoch 183/499\n",
            "----------\n",
            "train Loss: 5.0480 Acc: 0.7330\n",
            "val Loss: 4.5381 Acc: 0.7371\n",
            "\n",
            "Epoch 184/499\n",
            "----------\n",
            "train Loss: 5.0941 Acc: 0.7228\n",
            "val Loss: 2.7827 Acc: 0.7719\n",
            "\n",
            "Epoch 185/499\n",
            "----------\n",
            "train Loss: 2.2597 Acc: 0.8171\n",
            "val Loss: 4.3778 Acc: 0.7620\n",
            "\n",
            "Epoch 186/499\n",
            "----------\n",
            "train Loss: 2.2264 Acc: 0.8186\n",
            "val Loss: 2.1705 Acc: 0.8120\n",
            "\n",
            "Epoch 187/499\n",
            "----------\n",
            "train Loss: 2.3720 Acc: 0.8062\n",
            "val Loss: 15.5297 Acc: 0.6137\n",
            "\n",
            "Epoch 188/499\n",
            "----------\n",
            "train Loss: 7.0254 Acc: 0.6900\n",
            "val Loss: 4.4793 Acc: 0.6886\n",
            "\n",
            "Epoch 189/499\n",
            "----------\n",
            "train Loss: 3.7505 Acc: 0.7560\n",
            "val Loss: 37.5203 Acc: 0.3202\n",
            "\n",
            "Epoch 190/499\n",
            "----------\n",
            "train Loss: 9.8310 Acc: 0.6571\n",
            "val Loss: 5.2217 Acc: 0.7492\n",
            "\n",
            "Epoch 191/499\n",
            "----------\n",
            "train Loss: 2.4873 Acc: 0.8090\n",
            "val Loss: 6.1755 Acc: 0.7657\n",
            "\n",
            "Epoch 192/499\n",
            "----------\n",
            "train Loss: 2.4649 Acc: 0.8004\n",
            "val Loss: 6.0179 Acc: 0.6515\n",
            "\n",
            "Epoch 193/499\n",
            "----------\n",
            "train Loss: 3.7507 Acc: 0.7604\n",
            "val Loss: 6.7934 Acc: 0.6155\n",
            "\n",
            "Epoch 194/499\n",
            "----------\n",
            "train Loss: 2.8597 Acc: 0.7889\n",
            "val Loss: 9.8055 Acc: 0.6577\n",
            "\n",
            "Epoch 195/499\n",
            "----------\n",
            "train Loss: 3.9421 Acc: 0.7531\n",
            "val Loss: 9.4704 Acc: 0.5259\n",
            "\n",
            "Epoch 196/499\n",
            "----------\n",
            "train Loss: 4.0630 Acc: 0.7456\n",
            "val Loss: 5.7791 Acc: 0.7213\n",
            "\n",
            "Epoch 197/499\n",
            "----------\n",
            "train Loss: 2.4997 Acc: 0.8069\n",
            "val Loss: 4.8934 Acc: 0.6816\n",
            "\n",
            "Epoch 198/499\n",
            "----------\n",
            "train Loss: 2.6862 Acc: 0.7947\n",
            "val Loss: 2.7279 Acc: 0.8245\n",
            "\n",
            "Epoch 199/499\n",
            "----------\n",
            "train Loss: 2.5385 Acc: 0.8169\n",
            "val Loss: 3.4127 Acc: 0.8002\n",
            "\n",
            "Epoch 200/499\n",
            "----------\n",
            "train Loss: 2.2202 Acc: 0.8074\n",
            "val Loss: 8.4051 Acc: 0.7091\n",
            "\n",
            "Epoch 201/499\n",
            "----------\n",
            "train Loss: 5.4192 Acc: 0.7352\n",
            "val Loss: 31.8825 Acc: 0.4624\n",
            "\n",
            "Epoch 202/499\n",
            "----------\n",
            "train Loss: 10.1290 Acc: 0.6787\n",
            "val Loss: 5.7065 Acc: 0.7517\n",
            "\n",
            "Epoch 203/499\n",
            "----------\n",
            "train Loss: 2.5874 Acc: 0.8024\n",
            "val Loss: 3.8433 Acc: 0.7154\n",
            "\n",
            "Epoch 204/499\n",
            "----------\n",
            "train Loss: 2.6110 Acc: 0.8060\n",
            "val Loss: 1.4313 Acc: 0.8538\n",
            "\n",
            "Epoch 205/499\n",
            "----------\n",
            "train Loss: 2.3524 Acc: 0.8063\n",
            "val Loss: 16.9815 Acc: 0.6210\n",
            "\n",
            "Epoch 206/499\n",
            "----------\n",
            "train Loss: 4.8725 Acc: 0.7621\n",
            "val Loss: 2.1236 Acc: 0.7958\n",
            "\n",
            "Epoch 207/499\n",
            "----------\n",
            "train Loss: 1.8692 Acc: 0.8251\n",
            "val Loss: 5.1155 Acc: 0.7345\n",
            "\n",
            "Epoch 208/499\n",
            "----------\n",
            "train Loss: 4.0044 Acc: 0.7410\n",
            "val Loss: 2.7491 Acc: 0.8256\n",
            "\n",
            "Epoch 209/499\n",
            "----------\n",
            "train Loss: 2.6069 Acc: 0.7956\n",
            "val Loss: 7.9201 Acc: 0.5865\n",
            "\n",
            "Epoch 210/499\n",
            "----------\n",
            "train Loss: 3.6783 Acc: 0.7663\n",
            "val Loss: 4.6823 Acc: 0.7349\n",
            "\n",
            "Epoch 211/499\n",
            "----------\n",
            "train Loss: 2.2357 Acc: 0.8153\n",
            "val Loss: 1.8158 Acc: 0.8627\n",
            "\n",
            "Epoch 212/499\n",
            "----------\n",
            "train Loss: 2.8256 Acc: 0.7840\n",
            "val Loss: 16.1915 Acc: 0.5652\n",
            "\n",
            "Epoch 213/499\n",
            "----------\n",
            "train Loss: 3.4516 Acc: 0.7787\n",
            "val Loss: 8.8765 Acc: 0.6390\n",
            "\n",
            "Epoch 214/499\n",
            "----------\n",
            "train Loss: 3.7490 Acc: 0.7599\n",
            "val Loss: 3.4928 Acc: 0.7918\n",
            "\n",
            "Epoch 215/499\n",
            "----------\n",
            "train Loss: 3.5185 Acc: 0.7772\n",
            "val Loss: 3.2221 Acc: 0.7580\n",
            "\n",
            "Epoch 216/499\n",
            "----------\n",
            "train Loss: 2.5888 Acc: 0.8077\n",
            "val Loss: 10.0158 Acc: 0.6790\n",
            "\n",
            "Epoch 217/499\n",
            "----------\n",
            "train Loss: 3.6326 Acc: 0.7707\n",
            "val Loss: 2.3034 Acc: 0.8197\n",
            "\n",
            "Epoch 218/499\n",
            "----------\n",
            "train Loss: 2.4922 Acc: 0.8101\n",
            "val Loss: 11.4724 Acc: 0.5813\n",
            "\n",
            "Epoch 219/499\n",
            "----------\n",
            "train Loss: 6.4210 Acc: 0.6900\n",
            "val Loss: 9.5442 Acc: 0.5351\n",
            "\n",
            "Epoch 220/499\n",
            "----------\n",
            "train Loss: 3.1062 Acc: 0.7963\n",
            "val Loss: 1.2475 Acc: 0.8715\n",
            "\n",
            "Epoch 221/499\n",
            "----------\n",
            "train Loss: 2.2763 Acc: 0.8125\n",
            "val Loss: 25.7002 Acc: 0.4793\n",
            "\n",
            "Epoch 222/499\n",
            "----------\n",
            "train Loss: 5.8024 Acc: 0.7353\n",
            "val Loss: 16.2135 Acc: 0.6074\n",
            "\n",
            "Epoch 223/499\n",
            "----------\n",
            "train Loss: 4.7931 Acc: 0.7461\n",
            "val Loss: 5.1662 Acc: 0.6967\n",
            "\n",
            "Epoch 224/499\n",
            "----------\n",
            "train Loss: 3.2244 Acc: 0.7853\n",
            "val Loss: 2.3464 Acc: 0.7789\n",
            "\n",
            "Epoch 225/499\n",
            "----------\n",
            "train Loss: 2.1763 Acc: 0.8215\n",
            "val Loss: 4.0572 Acc: 0.7268\n",
            "\n",
            "Epoch 226/499\n",
            "----------\n",
            "train Loss: 3.7834 Acc: 0.7630\n",
            "val Loss: 10.6001 Acc: 0.6519\n",
            "\n",
            "Epoch 227/499\n",
            "----------\n",
            "train Loss: 3.2515 Acc: 0.7930\n",
            "val Loss: 5.6491 Acc: 0.6386\n",
            "\n",
            "Epoch 228/499\n",
            "----------\n",
            "train Loss: 2.7789 Acc: 0.8048\n",
            "val Loss: 6.9768 Acc: 0.7418\n",
            "\n",
            "Epoch 229/499\n",
            "----------\n",
            "train Loss: 4.0343 Acc: 0.7648\n",
            "val Loss: 2.2615 Acc: 0.8458\n",
            "\n",
            "Epoch 230/499\n",
            "----------\n",
            "train Loss: 2.2179 Acc: 0.8243\n",
            "val Loss: 5.4957 Acc: 0.6636\n",
            "\n",
            "Epoch 231/499\n",
            "----------\n",
            "train Loss: 3.0065 Acc: 0.7905\n",
            "val Loss: 1.1572 Acc: 0.8759\n",
            "\n",
            "Epoch 232/499\n",
            "----------\n",
            "train Loss: 2.1812 Acc: 0.8246\n",
            "val Loss: 5.2574 Acc: 0.7066\n",
            "\n",
            "Epoch 233/499\n",
            "----------\n",
            "train Loss: 2.2165 Acc: 0.8183\n",
            "val Loss: 10.4525 Acc: 0.6665\n",
            "\n",
            "Epoch 234/499\n",
            "----------\n",
            "train Loss: 3.9797 Acc: 0.7680\n",
            "val Loss: 4.4150 Acc: 0.6911\n",
            "\n",
            "Epoch 235/499\n",
            "----------\n",
            "train Loss: 3.8844 Acc: 0.7539\n",
            "val Loss: 16.5178 Acc: 0.5189\n",
            "\n",
            "Epoch 236/499\n",
            "----------\n",
            "train Loss: 2.8504 Acc: 0.7943\n",
            "val Loss: 3.1217 Acc: 0.7558\n",
            "\n",
            "Epoch 237/499\n",
            "----------\n",
            "train Loss: 2.7757 Acc: 0.7995\n",
            "val Loss: 3.1947 Acc: 0.7797\n",
            "\n",
            "Epoch 238/499\n",
            "----------\n",
            "train Loss: 4.1344 Acc: 0.7622\n",
            "val Loss: 4.9381 Acc: 0.6911\n",
            "\n",
            "Epoch 239/499\n",
            "----------\n",
            "train Loss: 2.1740 Acc: 0.8258\n",
            "val Loss: 1.4431 Acc: 0.8733\n",
            "\n",
            "Epoch 240/499\n",
            "----------\n",
            "train Loss: 2.9297 Acc: 0.7920\n",
            "val Loss: 5.2683 Acc: 0.7466\n",
            "\n",
            "Epoch 241/499\n",
            "----------\n",
            "train Loss: 4.3444 Acc: 0.7596\n",
            "val Loss: 24.6352 Acc: 0.4873\n",
            "\n",
            "Epoch 242/499\n",
            "----------\n",
            "train Loss: 7.6298 Acc: 0.7204\n",
            "val Loss: 3.0687 Acc: 0.7786\n",
            "\n",
            "Epoch 243/499\n",
            "----------\n",
            "train Loss: 2.3448 Acc: 0.8194\n",
            "val Loss: 1.4199 Acc: 0.8656\n",
            "\n",
            "Epoch 244/499\n",
            "----------\n",
            "train Loss: 2.2920 Acc: 0.8201\n",
            "val Loss: 4.3847 Acc: 0.6897\n",
            "\n",
            "Epoch 245/499\n",
            "----------\n",
            "train Loss: 2.3780 Acc: 0.8215\n",
            "val Loss: 1.1940 Acc: 0.8700\n",
            "\n",
            "Epoch 246/499\n",
            "----------\n",
            "train Loss: 2.4897 Acc: 0.8057\n",
            "val Loss: 2.4182 Acc: 0.7668\n",
            "\n",
            "Epoch 247/499\n",
            "----------\n",
            "train Loss: 1.9631 Acc: 0.8305\n",
            "val Loss: 6.3194 Acc: 0.6104\n",
            "\n",
            "Epoch 248/499\n",
            "----------\n",
            "train Loss: 4.9023 Acc: 0.7492\n",
            "val Loss: 3.9440 Acc: 0.7191\n",
            "\n",
            "Epoch 249/499\n",
            "----------\n",
            "train Loss: 3.4059 Acc: 0.7830\n",
            "val Loss: 3.9981 Acc: 0.7194\n",
            "\n",
            "Epoch 250/499\n",
            "----------\n",
            "train Loss: 3.9702 Acc: 0.7678\n",
            "val Loss: 9.4523 Acc: 0.6405\n",
            "\n",
            "Epoch 251/499\n",
            "----------\n",
            "train Loss: 4.5200 Acc: 0.7604\n",
            "val Loss: 10.1598 Acc: 0.6048\n",
            "\n",
            "Epoch 252/499\n",
            "----------\n",
            "train Loss: 4.9829 Acc: 0.7318\n",
            "val Loss: 7.1121 Acc: 0.7271\n",
            "\n",
            "Epoch 253/499\n",
            "----------\n",
            "train Loss: 4.7080 Acc: 0.7645\n",
            "val Loss: 1.9563 Acc: 0.8369\n",
            "\n",
            "Epoch 254/499\n",
            "----------\n",
            "train Loss: 1.8192 Acc: 0.8409\n",
            "val Loss: 3.0654 Acc: 0.7697\n",
            "\n",
            "Epoch 255/499\n",
            "----------\n",
            "train Loss: 2.2246 Acc: 0.8243\n",
            "val Loss: 1.1627 Acc: 0.8869\n",
            "\n",
            "Epoch 256/499\n",
            "----------\n",
            "train Loss: 2.0815 Acc: 0.8348\n",
            "val Loss: 1.8910 Acc: 0.8329\n",
            "\n",
            "Epoch 257/499\n",
            "----------\n",
            "train Loss: 2.2698 Acc: 0.8216\n",
            "val Loss: 2.9013 Acc: 0.7598\n",
            "\n",
            "Epoch 258/499\n",
            "----------\n",
            "train Loss: 3.7064 Acc: 0.7638\n",
            "val Loss: 7.0062 Acc: 0.6665\n",
            "\n",
            "Epoch 259/499\n",
            "----------\n",
            "train Loss: 2.1069 Acc: 0.8271\n",
            "val Loss: 1.4512 Acc: 0.8465\n",
            "\n",
            "Epoch 260/499\n",
            "----------\n",
            "train Loss: 2.9794 Acc: 0.8073\n",
            "val Loss: 11.0725 Acc: 0.6052\n",
            "\n",
            "Epoch 261/499\n",
            "----------\n",
            "train Loss: 3.4362 Acc: 0.7840\n",
            "val Loss: 8.9849 Acc: 0.6647\n",
            "\n",
            "Epoch 262/499\n",
            "----------\n",
            "train Loss: 3.3362 Acc: 0.7780\n",
            "val Loss: 3.8890 Acc: 0.7224\n",
            "\n",
            "Epoch 263/499\n",
            "----------\n",
            "train Loss: 1.9870 Acc: 0.8350\n",
            "val Loss: 5.3755 Acc: 0.6625\n",
            "\n",
            "Epoch 264/499\n",
            "----------\n",
            "train Loss: 2.3202 Acc: 0.8114\n",
            "val Loss: 3.5723 Acc: 0.7466\n",
            "\n",
            "Epoch 265/499\n",
            "----------\n",
            "train Loss: 2.7459 Acc: 0.8010\n",
            "val Loss: 3.5593 Acc: 0.7973\n",
            "\n",
            "Epoch 266/499\n",
            "----------\n",
            "train Loss: 2.0465 Acc: 0.8247\n",
            "val Loss: 1.7276 Acc: 0.8447\n",
            "\n",
            "Epoch 267/499\n",
            "----------\n",
            "train Loss: 2.1363 Acc: 0.8245\n",
            "val Loss: 1.7728 Acc: 0.8300\n",
            "\n",
            "Epoch 268/499\n",
            "----------\n",
            "train Loss: 2.2464 Acc: 0.8264\n",
            "val Loss: 3.3969 Acc: 0.7437\n",
            "\n",
            "Epoch 269/499\n",
            "----------\n",
            "train Loss: 2.7663 Acc: 0.7931\n",
            "val Loss: 9.6802 Acc: 0.6335\n",
            "\n",
            "Epoch 270/499\n",
            "----------\n",
            "train Loss: 5.3972 Acc: 0.7170\n",
            "val Loss: 1.8872 Acc: 0.8505\n",
            "\n",
            "Epoch 271/499\n",
            "----------\n",
            "train Loss: 1.7139 Acc: 0.8460\n",
            "val Loss: 1.2137 Acc: 0.8685\n",
            "\n",
            "Epoch 272/499\n",
            "----------\n",
            "train Loss: 3.2180 Acc: 0.7891\n",
            "val Loss: 22.3682 Acc: 0.5281\n",
            "\n",
            "Epoch 273/499\n",
            "----------\n",
            "train Loss: 4.5976 Acc: 0.7590\n",
            "val Loss: 3.5201 Acc: 0.8072\n",
            "\n",
            "Epoch 274/499\n",
            "----------\n",
            "train Loss: 2.3629 Acc: 0.8147\n",
            "val Loss: 6.2422 Acc: 0.7271\n",
            "\n",
            "Epoch 275/499\n",
            "----------\n",
            "train Loss: 3.4009 Acc: 0.7737\n",
            "val Loss: 16.1151 Acc: 0.4748\n",
            "\n",
            "Epoch 276/499\n",
            "----------\n",
            "train Loss: 5.6265 Acc: 0.7372\n",
            "val Loss: 3.8628 Acc: 0.7863\n",
            "\n",
            "Epoch 277/499\n",
            "----------\n",
            "train Loss: 2.1226 Acc: 0.8389\n",
            "val Loss: 3.6192 Acc: 0.7517\n",
            "\n",
            "Epoch 278/499\n",
            "----------\n",
            "train Loss: 3.3993 Acc: 0.7836\n",
            "val Loss: 13.6610 Acc: 0.5725\n",
            "\n",
            "Epoch 279/499\n",
            "----------\n",
            "train Loss: 3.7821 Acc: 0.7675\n",
            "val Loss: 2.5301 Acc: 0.8024\n",
            "\n",
            "Epoch 280/499\n",
            "----------\n",
            "train Loss: 2.3115 Acc: 0.8190\n",
            "val Loss: 6.4496 Acc: 0.7180\n",
            "\n",
            "Epoch 281/499\n",
            "----------\n",
            "train Loss: 4.1747 Acc: 0.7619\n",
            "val Loss: 4.1937 Acc: 0.7088\n",
            "\n",
            "Epoch 282/499\n",
            "----------\n",
            "train Loss: 2.8420 Acc: 0.7987\n",
            "val Loss: 7.0733 Acc: 0.7345\n",
            "\n",
            "Epoch 283/499\n",
            "----------\n",
            "train Loss: 2.3024 Acc: 0.8232\n",
            "val Loss: 4.2786 Acc: 0.7330\n",
            "\n",
            "Epoch 284/499\n",
            "----------\n",
            "train Loss: 1.7706 Acc: 0.8407\n",
            "val Loss: 1.5289 Acc: 0.8491\n",
            "\n",
            "Epoch 285/499\n",
            "----------\n",
            "train Loss: 1.5517 Acc: 0.8493\n",
            "val Loss: 3.1108 Acc: 0.7661\n",
            "\n",
            "Epoch 286/499\n",
            "----------\n",
            "train Loss: 2.8430 Acc: 0.8000\n",
            "val Loss: 15.8807 Acc: 0.5314\n",
            "\n",
            "Epoch 287/499\n",
            "----------\n",
            "train Loss: 3.5034 Acc: 0.7822\n",
            "val Loss: 8.3652 Acc: 0.6383\n",
            "\n",
            "Epoch 288/499\n",
            "----------\n",
            "train Loss: 2.7527 Acc: 0.8006\n",
            "val Loss: 1.4740 Acc: 0.8575\n",
            "\n",
            "Epoch 289/499\n",
            "----------\n",
            "train Loss: 2.2475 Acc: 0.8226\n",
            "val Loss: 2.2131 Acc: 0.8145\n",
            "\n",
            "Epoch 290/499\n",
            "----------\n",
            "train Loss: 2.5546 Acc: 0.8135\n",
            "val Loss: 2.6892 Acc: 0.8373\n",
            "\n",
            "Epoch 291/499\n",
            "----------\n",
            "train Loss: 2.4054 Acc: 0.8067\n",
            "val Loss: 8.6069 Acc: 0.7411\n",
            "\n",
            "Epoch 292/499\n",
            "----------\n",
            "train Loss: 2.8339 Acc: 0.7924\n",
            "val Loss: 14.9429 Acc: 0.4965\n",
            "\n",
            "Epoch 293/499\n",
            "----------\n",
            "train Loss: 5.4551 Acc: 0.7372\n",
            "val Loss: 3.2281 Acc: 0.8156\n",
            "\n",
            "Epoch 294/499\n",
            "----------\n",
            "train Loss: 2.0752 Acc: 0.8313\n",
            "val Loss: 4.4771 Acc: 0.7936\n",
            "\n",
            "Epoch 295/499\n",
            "----------\n",
            "train Loss: 2.3604 Acc: 0.8149\n",
            "val Loss: 1.5078 Acc: 0.8513\n",
            "\n",
            "Epoch 296/499\n",
            "----------\n",
            "train Loss: 1.8095 Acc: 0.8351\n",
            "val Loss: 1.7563 Acc: 0.8402\n",
            "\n",
            "Epoch 297/499\n",
            "----------\n",
            "train Loss: 2.1223 Acc: 0.8185\n",
            "val Loss: 3.0703 Acc: 0.7690\n",
            "\n",
            "Epoch 298/499\n",
            "----------\n",
            "train Loss: 3.1721 Acc: 0.7862\n",
            "val Loss: 10.8872 Acc: 0.5336\n",
            "\n",
            "Epoch 299/499\n",
            "----------\n",
            "train Loss: 3.2206 Acc: 0.7847\n",
            "val Loss: 9.9666 Acc: 0.6056\n",
            "\n",
            "Epoch 300/499\n",
            "----------\n",
            "train Loss: 2.7925 Acc: 0.8071\n",
            "val Loss: 2.5685 Acc: 0.8072\n",
            "\n",
            "Epoch 301/499\n",
            "----------\n",
            "train Loss: 1.8741 Acc: 0.8432\n",
            "val Loss: 2.7333 Acc: 0.8402\n",
            "\n",
            "Epoch 302/499\n",
            "----------\n",
            "train Loss: 2.0703 Acc: 0.8299\n",
            "val Loss: 9.5350 Acc: 0.6232\n",
            "\n",
            "Epoch 303/499\n",
            "----------\n",
            "train Loss: 4.6371 Acc: 0.7430\n",
            "val Loss: 6.9775 Acc: 0.6298\n",
            "\n",
            "Epoch 304/499\n",
            "----------\n",
            "train Loss: 1.8824 Acc: 0.8387\n",
            "val Loss: 5.7019 Acc: 0.6654\n",
            "\n",
            "Epoch 305/499\n",
            "----------\n",
            "train Loss: 2.0408 Acc: 0.8203\n",
            "val Loss: 6.7600 Acc: 0.6548\n",
            "\n",
            "Epoch 306/499\n",
            "----------\n",
            "train Loss: 2.3584 Acc: 0.8113\n",
            "val Loss: 2.0572 Acc: 0.8145\n",
            "\n",
            "Epoch 307/499\n",
            "----------\n",
            "train Loss: 2.3783 Acc: 0.8148\n",
            "val Loss: 1.9987 Acc: 0.8167\n",
            "\n",
            "Epoch 308/499\n",
            "----------\n",
            "train Loss: 1.8761 Acc: 0.8316\n",
            "val Loss: 2.2538 Acc: 0.8153\n",
            "\n",
            "Epoch 309/499\n",
            "----------\n",
            "train Loss: 1.9373 Acc: 0.8314\n",
            "val Loss: 1.4931 Acc: 0.8638\n",
            "\n",
            "Epoch 310/499\n",
            "----------\n",
            "train Loss: 2.7703 Acc: 0.8015\n",
            "val Loss: 16.1691 Acc: 0.5288\n",
            "\n",
            "Epoch 311/499\n",
            "----------\n",
            "train Loss: 5.1376 Acc: 0.7266\n",
            "val Loss: 5.7415 Acc: 0.6456\n",
            "\n",
            "Epoch 312/499\n",
            "----------\n",
            "train Loss: 2.4980 Acc: 0.8061\n",
            "val Loss: 2.5756 Acc: 0.8087\n",
            "\n",
            "Epoch 313/499\n",
            "----------\n",
            "train Loss: 2.6656 Acc: 0.8161\n",
            "val Loss: 4.6358 Acc: 0.7058\n",
            "\n",
            "Epoch 314/499\n",
            "----------\n",
            "train Loss: 2.7687 Acc: 0.7968\n",
            "val Loss: 2.7659 Acc: 0.7859\n",
            "\n",
            "Epoch 315/499\n",
            "----------\n",
            "train Loss: 1.7538 Acc: 0.8418\n",
            "val Loss: 2.4944 Acc: 0.8134\n",
            "\n",
            "Epoch 316/499\n",
            "----------\n",
            "train Loss: 1.8584 Acc: 0.8349\n",
            "val Loss: 2.8565 Acc: 0.7925\n",
            "\n",
            "Epoch 317/499\n",
            "----------\n",
            "train Loss: 2.3810 Acc: 0.8109\n",
            "val Loss: 3.0609 Acc: 0.7506\n",
            "\n",
            "Epoch 318/499\n",
            "----------\n",
            "train Loss: 3.8082 Acc: 0.7628\n",
            "val Loss: 4.8016 Acc: 0.7470\n",
            "\n",
            "Epoch 319/499\n",
            "----------\n",
            "train Loss: 2.3921 Acc: 0.8090\n",
            "val Loss: 3.6657 Acc: 0.7830\n",
            "\n",
            "Epoch 320/499\n",
            "----------\n",
            "train Loss: 2.2296 Acc: 0.8176\n",
            "val Loss: 1.8302 Acc: 0.8439\n",
            "\n",
            "Epoch 321/499\n",
            "----------\n",
            "train Loss: 2.3663 Acc: 0.8101\n",
            "val Loss: 4.4184 Acc: 0.7837\n",
            "\n",
            "Epoch 322/499\n",
            "----------\n",
            "train Loss: 2.7501 Acc: 0.7971\n",
            "val Loss: 1.7629 Acc: 0.8344\n",
            "\n",
            "Epoch 323/499\n",
            "----------\n",
            "train Loss: 2.1573 Acc: 0.8232\n",
            "val Loss: 4.4000 Acc: 0.6996\n",
            "\n",
            "Epoch 324/499\n",
            "----------\n",
            "train Loss: 2.1453 Acc: 0.8212\n",
            "val Loss: 3.3084 Acc: 0.7374\n",
            "\n",
            "Epoch 325/499\n",
            "----------\n",
            "train Loss: 1.8978 Acc: 0.8311\n",
            "val Loss: 8.7523 Acc: 0.5828\n",
            "\n",
            "Epoch 326/499\n",
            "----------\n",
            "train Loss: 3.6007 Acc: 0.7591\n",
            "val Loss: 3.3175 Acc: 0.7899\n",
            "\n",
            "Epoch 327/499\n",
            "----------\n",
            "train Loss: 1.8190 Acc: 0.8411\n",
            "val Loss: 3.3365 Acc: 0.7716\n",
            "\n",
            "Epoch 328/499\n",
            "----------\n",
            "train Loss: 2.1077 Acc: 0.8237\n",
            "val Loss: 1.8035 Acc: 0.8263\n",
            "\n",
            "Epoch 329/499\n",
            "----------\n",
            "train Loss: 2.5805 Acc: 0.7943\n",
            "val Loss: 4.3800 Acc: 0.7033\n",
            "\n",
            "Epoch 330/499\n",
            "----------\n",
            "train Loss: 2.0124 Acc: 0.8274\n",
            "val Loss: 1.6594 Acc: 0.8204\n",
            "\n",
            "Epoch 331/499\n",
            "----------\n",
            "train Loss: 2.5165 Acc: 0.8046\n",
            "val Loss: 7.8483 Acc: 0.6911\n",
            "\n",
            "Epoch 332/499\n",
            "----------\n",
            "train Loss: 2.9717 Acc: 0.7855\n",
            "val Loss: 1.8555 Acc: 0.8189\n",
            "\n",
            "Epoch 333/499\n",
            "----------\n",
            "train Loss: 1.8873 Acc: 0.8359\n",
            "val Loss: 3.2817 Acc: 0.7106\n",
            "\n",
            "Epoch 334/499\n",
            "----------\n",
            "train Loss: 2.4493 Acc: 0.8002\n",
            "val Loss: 5.6918 Acc: 0.7007\n",
            "\n",
            "Epoch 335/499\n",
            "----------\n",
            "train Loss: 3.0053 Acc: 0.7868\n",
            "val Loss: 3.8937 Acc: 0.6798\n",
            "\n",
            "Epoch 336/499\n",
            "----------\n",
            "train Loss: 1.8200 Acc: 0.8365\n",
            "val Loss: 2.2958 Acc: 0.8296\n",
            "\n",
            "Epoch 337/499\n",
            "----------\n",
            "train Loss: 1.9485 Acc: 0.8326\n",
            "val Loss: 3.3928 Acc: 0.7701\n",
            "\n",
            "Epoch 338/499\n",
            "----------\n",
            "train Loss: 4.3345 Acc: 0.7548\n",
            "val Loss: 6.9162 Acc: 0.7036\n",
            "\n",
            "Epoch 339/499\n",
            "----------\n",
            "train Loss: 2.9140 Acc: 0.7947\n",
            "val Loss: 2.6460 Acc: 0.8109\n",
            "\n",
            "Epoch 340/499\n",
            "----------\n",
            "train Loss: 2.6923 Acc: 0.8012\n",
            "val Loss: 1.6998 Acc: 0.8333\n",
            "\n",
            "Epoch 341/499\n",
            "----------\n",
            "train Loss: 2.1561 Acc: 0.8162\n",
            "val Loss: 1.9966 Acc: 0.8281\n",
            "\n",
            "Epoch 342/499\n",
            "----------\n",
            "train Loss: 2.0241 Acc: 0.8215\n",
            "val Loss: 5.3616 Acc: 0.7011\n",
            "\n",
            "Epoch 343/499\n",
            "----------\n",
            "train Loss: 2.0701 Acc: 0.8251\n",
            "val Loss: 3.1551 Acc: 0.7668\n",
            "\n",
            "Epoch 344/499\n",
            "----------\n",
            "train Loss: 1.8663 Acc: 0.8335\n",
            "val Loss: 1.3235 Acc: 0.8612\n",
            "\n",
            "Epoch 345/499\n",
            "----------\n",
            "train Loss: 2.2508 Acc: 0.8106\n",
            "val Loss: 10.8424 Acc: 0.5428\n",
            "\n",
            "Epoch 346/499\n",
            "----------\n",
            "train Loss: 4.5417 Acc: 0.7451\n",
            "val Loss: 1.4651 Acc: 0.8472\n",
            "\n",
            "Epoch 347/499\n",
            "----------\n",
            "train Loss: 1.8508 Acc: 0.8339\n",
            "val Loss: 4.8764 Acc: 0.7205\n",
            "\n",
            "Epoch 348/499\n",
            "----------\n",
            "train Loss: 2.1104 Acc: 0.8307\n",
            "val Loss: 2.4030 Acc: 0.7932\n",
            "\n",
            "Epoch 349/499\n",
            "----------\n",
            "train Loss: 3.2927 Acc: 0.7758\n",
            "val Loss: 4.5201 Acc: 0.7183\n",
            "\n",
            "Epoch 350/499\n",
            "----------\n",
            "train Loss: 4.3101 Acc: 0.7493\n",
            "val Loss: 3.8485 Acc: 0.7466\n",
            "\n",
            "Epoch 351/499\n",
            "----------\n",
            "train Loss: 2.7071 Acc: 0.8060\n",
            "val Loss: 3.4279 Acc: 0.7749\n",
            "\n",
            "Epoch 352/499\n",
            "----------\n",
            "train Loss: 1.9665 Acc: 0.8348\n",
            "val Loss: 7.9783 Acc: 0.5810\n",
            "\n",
            "Epoch 353/499\n",
            "----------\n",
            "train Loss: 3.4401 Acc: 0.7808\n",
            "val Loss: 3.9030 Acc: 0.7694\n",
            "\n",
            "Epoch 354/499\n",
            "----------\n",
            "train Loss: 1.8369 Acc: 0.8273\n",
            "val Loss: 5.5756 Acc: 0.6610\n",
            "\n",
            "Epoch 355/499\n",
            "----------\n",
            "train Loss: 3.2055 Acc: 0.7802\n",
            "val Loss: 1.6932 Acc: 0.8131\n",
            "\n",
            "Epoch 356/499\n",
            "----------\n",
            "train Loss: 1.5095 Acc: 0.8506\n",
            "val Loss: 6.4232 Acc: 0.7363\n",
            "\n",
            "Epoch 357/499\n",
            "----------\n",
            "train Loss: 2.4359 Acc: 0.8126\n",
            "val Loss: 1.8384 Acc: 0.8142\n",
            "\n",
            "Epoch 358/499\n",
            "----------\n",
            "train Loss: 2.1978 Acc: 0.8118\n",
            "val Loss: 4.3112 Acc: 0.7749\n",
            "\n",
            "Epoch 359/499\n",
            "----------\n",
            "train Loss: 1.8854 Acc: 0.8331\n",
            "val Loss: 10.6977 Acc: 0.6427\n",
            "\n",
            "Epoch 360/499\n",
            "----------\n",
            "train Loss: 4.7858 Acc: 0.7275\n",
            "val Loss: 9.0919 Acc: 0.7958\n",
            "\n",
            "Epoch 361/499\n",
            "----------\n",
            "train Loss: 3.0606 Acc: 0.8072\n",
            "val Loss: 2.8624 Acc: 0.7411\n",
            "\n",
            "Epoch 362/499\n",
            "----------\n",
            "train Loss: 1.5350 Acc: 0.8487\n",
            "val Loss: 4.6151 Acc: 0.7260\n",
            "\n",
            "Epoch 363/499\n",
            "----------\n",
            "train Loss: 2.2756 Acc: 0.8158\n",
            "val Loss: 4.3870 Acc: 0.6643\n",
            "\n",
            "Epoch 364/499\n",
            "----------\n",
            "train Loss: 2.7974 Acc: 0.7900\n",
            "val Loss: 2.1380 Acc: 0.8241\n",
            "\n",
            "Epoch 365/499\n",
            "----------\n",
            "train Loss: 1.8378 Acc: 0.8340\n",
            "val Loss: 3.0356 Acc: 0.7547\n",
            "\n",
            "Epoch 366/499\n",
            "----------\n",
            "train Loss: 1.6973 Acc: 0.8412\n",
            "val Loss: 1.8493 Acc: 0.8248\n",
            "\n",
            "Epoch 367/499\n",
            "----------\n",
            "train Loss: 1.9579 Acc: 0.8293\n",
            "val Loss: 1.9709 Acc: 0.8292\n",
            "\n",
            "Epoch 368/499\n",
            "----------\n",
            "train Loss: 1.5021 Acc: 0.8486\n",
            "val Loss: 3.2627 Acc: 0.7117\n",
            "\n",
            "Epoch 369/499\n",
            "----------\n",
            "train Loss: 2.0684 Acc: 0.8124\n",
            "val Loss: 7.6848 Acc: 0.6485\n",
            "\n",
            "Epoch 370/499\n",
            "----------\n",
            "train Loss: 2.3231 Acc: 0.8027\n",
            "val Loss: 4.4927 Acc: 0.7047\n",
            "\n",
            "Epoch 371/499\n",
            "----------\n",
            "train Loss: 2.3585 Acc: 0.8047\n",
            "val Loss: 2.4367 Acc: 0.7499\n",
            "\n",
            "Epoch 372/499\n",
            "----------\n",
            "train Loss: 1.8579 Acc: 0.8324\n",
            "val Loss: 6.2349 Acc: 0.6276\n",
            "\n",
            "Epoch 373/499\n",
            "----------\n",
            "train Loss: 2.2414 Acc: 0.8080\n",
            "val Loss: 2.0846 Acc: 0.8061\n",
            "\n",
            "Epoch 374/499\n",
            "----------\n",
            "train Loss: 1.8914 Acc: 0.8310\n",
            "val Loss: 2.6872 Acc: 0.7752\n",
            "\n",
            "Epoch 375/499\n",
            "----------\n",
            "train Loss: 2.5051 Acc: 0.7921\n",
            "val Loss: 3.8237 Acc: 0.6878\n",
            "\n",
            "Epoch 376/499\n",
            "----------\n",
            "train Loss: 2.0300 Acc: 0.8180\n",
            "val Loss: 3.0800 Acc: 0.7510\n",
            "\n",
            "Epoch 377/499\n",
            "----------\n",
            "train Loss: 2.2846 Acc: 0.8005\n",
            "val Loss: 5.3313 Acc: 0.6669\n",
            "\n",
            "Epoch 378/499\n",
            "----------\n",
            "train Loss: 2.3996 Acc: 0.8107\n",
            "val Loss: 1.8156 Acc: 0.8175\n",
            "\n",
            "Epoch 379/499\n",
            "----------\n",
            "train Loss: 2.7284 Acc: 0.7838\n",
            "val Loss: 9.6709 Acc: 0.5707\n",
            "\n",
            "Epoch 380/499\n",
            "----------\n",
            "train Loss: 2.6262 Acc: 0.7929\n",
            "val Loss: 2.1497 Acc: 0.8010\n",
            "\n",
            "Epoch 381/499\n",
            "----------\n",
            "train Loss: 1.5487 Acc: 0.8494\n",
            "val Loss: 1.6768 Acc: 0.8432\n",
            "\n",
            "Epoch 382/499\n",
            "----------\n",
            "train Loss: 2.0664 Acc: 0.8155\n",
            "val Loss: 1.2774 Acc: 0.8538\n",
            "\n",
            "Epoch 383/499\n",
            "----------\n",
            "train Loss: 1.4350 Acc: 0.8519\n",
            "val Loss: 0.9101 Acc: 0.8784\n",
            "\n",
            "Epoch 384/499\n",
            "----------\n",
            "train Loss: 1.3504 Acc: 0.8608\n",
            "val Loss: 2.8586 Acc: 0.8043\n",
            "\n",
            "Epoch 385/499\n",
            "----------\n",
            "train Loss: 1.4073 Acc: 0.8534\n",
            "val Loss: 5.4095 Acc: 0.6882\n",
            "\n",
            "Epoch 386/499\n",
            "----------\n",
            "train Loss: 2.5406 Acc: 0.7904\n",
            "val Loss: 15.7096 Acc: 0.5905\n",
            "\n",
            "Epoch 387/499\n",
            "----------\n",
            "train Loss: 4.9136 Acc: 0.7412\n",
            "val Loss: 3.4038 Acc: 0.7154\n",
            "\n",
            "Epoch 388/499\n",
            "----------\n",
            "train Loss: 1.6698 Acc: 0.8421\n",
            "val Loss: 2.7323 Acc: 0.7235\n",
            "\n",
            "Epoch 389/499\n",
            "----------\n",
            "train Loss: 2.2601 Acc: 0.8179\n",
            "val Loss: 3.3036 Acc: 0.7371\n",
            "\n",
            "Epoch 390/499\n",
            "----------\n",
            "train Loss: 2.1897 Acc: 0.8081\n",
            "val Loss: 1.9969 Acc: 0.7958\n",
            "\n",
            "Epoch 391/499\n",
            "----------\n",
            "train Loss: 2.1216 Acc: 0.8184\n",
            "val Loss: 3.6863 Acc: 0.7811\n",
            "\n",
            "Epoch 392/499\n",
            "----------\n",
            "train Loss: 2.1735 Acc: 0.8122\n",
            "val Loss: 3.0519 Acc: 0.7363\n",
            "\n",
            "Epoch 393/499\n",
            "----------\n",
            "train Loss: 3.0361 Acc: 0.7732\n",
            "val Loss: 2.0694 Acc: 0.8178\n",
            "\n",
            "Epoch 394/499\n",
            "----------\n",
            "train Loss: 1.7906 Acc: 0.8362\n",
            "val Loss: 1.0707 Acc: 0.8810\n",
            "\n",
            "Epoch 395/499\n",
            "----------\n",
            "train Loss: 1.5888 Acc: 0.8403\n",
            "val Loss: 1.8701 Acc: 0.8259\n",
            "\n",
            "Epoch 396/499\n",
            "----------\n",
            "train Loss: 2.6614 Acc: 0.7814\n",
            "val Loss: 1.8643 Acc: 0.8017\n",
            "\n",
            "Epoch 397/499\n",
            "----------\n",
            "train Loss: 2.2555 Acc: 0.8080\n",
            "val Loss: 1.2493 Acc: 0.8395\n",
            "\n",
            "Epoch 398/499\n",
            "----------\n",
            "train Loss: 1.6841 Acc: 0.8321\n",
            "val Loss: 4.8130 Acc: 0.6401\n",
            "\n",
            "Epoch 399/499\n",
            "----------\n",
            "train Loss: 2.3218 Acc: 0.7985\n",
            "val Loss: 5.4403 Acc: 0.6574\n",
            "\n",
            "Epoch 400/499\n",
            "----------\n",
            "train Loss: 1.7069 Acc: 0.8383\n",
            "val Loss: 5.0412 Acc: 0.6629\n",
            "\n",
            "Epoch 401/499\n",
            "----------\n",
            "train Loss: 1.9998 Acc: 0.8226\n",
            "val Loss: 2.9604 Acc: 0.7595\n",
            "\n",
            "Epoch 402/499\n",
            "----------\n",
            "train Loss: 2.2850 Acc: 0.7986\n",
            "val Loss: 2.8034 Acc: 0.7741\n",
            "\n",
            "Epoch 403/499\n",
            "----------\n",
            "train Loss: 2.1342 Acc: 0.8141\n",
            "val Loss: 2.8375 Acc: 0.7800\n",
            "\n",
            "Epoch 404/499\n",
            "----------\n",
            "train Loss: 1.6675 Acc: 0.8393\n",
            "val Loss: 3.0297 Acc: 0.8303\n",
            "\n",
            "Epoch 405/499\n",
            "----------\n",
            "train Loss: 2.7729 Acc: 0.7941\n",
            "val Loss: 2.0897 Acc: 0.7910\n",
            "\n",
            "Epoch 406/499\n",
            "----------\n",
            "train Loss: 1.6575 Acc: 0.8361\n",
            "val Loss: 1.3972 Acc: 0.8531\n",
            "\n",
            "Epoch 407/499\n",
            "----------\n",
            "train Loss: 1.3289 Acc: 0.8559\n",
            "val Loss: 2.5015 Acc: 0.7216\n",
            "\n",
            "Epoch 408/499\n",
            "----------\n",
            "train Loss: 2.2773 Acc: 0.7982\n",
            "val Loss: 0.8652 Acc: 0.8891\n",
            "\n",
            "Epoch 409/499\n",
            "----------\n",
            "train Loss: 1.7179 Acc: 0.8309\n",
            "val Loss: 3.1955 Acc: 0.7330\n",
            "\n",
            "Epoch 410/499\n",
            "----------\n",
            "train Loss: 1.3448 Acc: 0.8479\n",
            "val Loss: 1.6244 Acc: 0.8362\n",
            "\n",
            "Epoch 411/499\n",
            "----------\n",
            "train Loss: 2.0898 Acc: 0.8068\n",
            "val Loss: 3.6654 Acc: 0.6864\n",
            "\n",
            "Epoch 412/499\n",
            "----------\n",
            "train Loss: 1.9933 Acc: 0.8110\n",
            "val Loss: 3.9043 Acc: 0.6768\n",
            "\n",
            "Epoch 413/499\n",
            "----------\n",
            "train Loss: 3.2387 Acc: 0.7653\n",
            "val Loss: 6.3530 Acc: 0.7264\n",
            "\n",
            "Epoch 414/499\n",
            "----------\n",
            "train Loss: 2.0862 Acc: 0.8297\n",
            "val Loss: 3.0518 Acc: 0.7587\n",
            "\n",
            "Epoch 415/499\n",
            "----------\n",
            "train Loss: 2.0394 Acc: 0.8125\n",
            "val Loss: 0.9784 Acc: 0.8715\n",
            "\n",
            "Epoch 416/499\n",
            "----------\n",
            "train Loss: 1.7702 Acc: 0.8300\n",
            "val Loss: 21.2448 Acc: 0.3867\n",
            "\n",
            "Epoch 417/499\n",
            "----------\n",
            "train Loss: 4.3085 Acc: 0.7412\n",
            "val Loss: 2.2254 Acc: 0.7969\n",
            "\n",
            "Epoch 418/499\n",
            "----------\n",
            "train Loss: 1.7794 Acc: 0.8160\n",
            "val Loss: 2.5234 Acc: 0.7932\n",
            "\n",
            "Epoch 419/499\n",
            "----------\n",
            "train Loss: 1.5733 Acc: 0.8351\n",
            "val Loss: 1.0060 Acc: 0.8619\n",
            "\n",
            "Epoch 420/499\n",
            "----------\n",
            "train Loss: 1.3219 Acc: 0.8497\n",
            "val Loss: 1.1031 Acc: 0.8513\n",
            "\n",
            "Epoch 421/499\n",
            "----------\n",
            "train Loss: 2.1197 Acc: 0.8158\n",
            "val Loss: 4.4793 Acc: 0.7396\n",
            "\n",
            "Epoch 422/499\n",
            "----------\n",
            "train Loss: 2.3855 Acc: 0.7973\n",
            "val Loss: 3.4065 Acc: 0.7194\n",
            "\n",
            "Epoch 423/499\n",
            "----------\n",
            "train Loss: 1.8534 Acc: 0.8146\n",
            "val Loss: 1.1578 Acc: 0.8689\n",
            "\n",
            "Epoch 424/499\n",
            "----------\n",
            "train Loss: 1.2630 Acc: 0.8560\n",
            "val Loss: 1.4763 Acc: 0.8344\n",
            "\n",
            "Epoch 425/499\n",
            "----------\n",
            "train Loss: 1.2213 Acc: 0.8569\n",
            "val Loss: 2.6834 Acc: 0.7312\n",
            "\n",
            "Epoch 426/499\n",
            "----------\n",
            "train Loss: 1.5834 Acc: 0.8319\n",
            "val Loss: 2.4803 Acc: 0.7991\n",
            "\n",
            "Epoch 427/499\n",
            "----------\n",
            "train Loss: 1.8828 Acc: 0.8182\n",
            "val Loss: 1.0570 Acc: 0.8414\n",
            "\n",
            "Epoch 428/499\n",
            "----------\n",
            "train Loss: 1.9665 Acc: 0.8141\n",
            "val Loss: 5.3903 Acc: 0.6585\n",
            "\n",
            "Epoch 429/499\n",
            "----------\n",
            "train Loss: 1.8766 Acc: 0.8221\n",
            "val Loss: 1.3608 Acc: 0.8219\n",
            "\n",
            "Epoch 430/499\n",
            "----------\n",
            "train Loss: 2.0783 Acc: 0.8035\n",
            "val Loss: 2.6183 Acc: 0.8079\n",
            "\n",
            "Epoch 431/499\n",
            "----------\n",
            "train Loss: 1.6960 Acc: 0.8238\n",
            "val Loss: 5.5861 Acc: 0.7521\n",
            "\n",
            "Epoch 432/499\n",
            "----------\n",
            "train Loss: 2.5867 Acc: 0.7894\n",
            "val Loss: 1.7133 Acc: 0.8149\n",
            "\n",
            "Epoch 433/499\n",
            "----------\n",
            "train Loss: 1.4535 Acc: 0.8404\n",
            "val Loss: 1.2229 Acc: 0.8380\n",
            "\n",
            "Epoch 434/499\n",
            "----------\n",
            "train Loss: 1.3014 Acc: 0.8476\n",
            "val Loss: 3.5591 Acc: 0.7110\n",
            "\n",
            "Epoch 435/499\n",
            "----------\n",
            "train Loss: 1.8684 Acc: 0.8129\n",
            "val Loss: 2.2508 Acc: 0.7756\n",
            "\n",
            "Epoch 436/499\n",
            "----------\n",
            "train Loss: 2.1163 Acc: 0.7990\n",
            "val Loss: 3.2041 Acc: 0.7470\n",
            "\n",
            "Epoch 437/499\n",
            "----------\n",
            "train Loss: 2.2643 Acc: 0.8048\n",
            "val Loss: 2.2096 Acc: 0.7617\n",
            "\n",
            "Epoch 438/499\n",
            "----------\n",
            "train Loss: 1.6751 Acc: 0.8356\n",
            "val Loss: 1.2336 Acc: 0.8340\n",
            "\n",
            "Epoch 439/499\n",
            "----------\n",
            "train Loss: 1.2337 Acc: 0.8519\n",
            "val Loss: 3.5052 Acc: 0.6945\n",
            "\n",
            "Epoch 440/499\n",
            "----------\n",
            "train Loss: 2.0793 Acc: 0.8169\n",
            "val Loss: 8.8718 Acc: 0.5079\n",
            "\n",
            "Epoch 441/499\n",
            "----------\n",
            "train Loss: 2.1881 Acc: 0.8119\n",
            "val Loss: 6.8315 Acc: 0.5876\n",
            "\n",
            "Epoch 442/499\n",
            "----------\n",
            "train Loss: 1.5403 Acc: 0.8395\n",
            "val Loss: 8.6035 Acc: 0.6096\n",
            "\n",
            "Epoch 443/499\n",
            "----------\n",
            "train Loss: 2.2066 Acc: 0.7995\n",
            "val Loss: 4.8022 Acc: 0.6317\n",
            "\n",
            "Epoch 444/499\n",
            "----------\n",
            "train Loss: 1.9096 Acc: 0.8104\n",
            "val Loss: 5.5587 Acc: 0.6721\n",
            "\n",
            "Epoch 445/499\n",
            "----------\n",
            "train Loss: 1.8508 Acc: 0.8177\n",
            "val Loss: 3.8194 Acc: 0.7003\n",
            "\n",
            "Epoch 446/499\n",
            "----------\n",
            "train Loss: 2.7201 Acc: 0.7783\n",
            "val Loss: 10.6143 Acc: 0.5968\n",
            "\n",
            "Epoch 447/499\n",
            "----------\n",
            "train Loss: 2.2673 Acc: 0.7957\n",
            "val Loss: 4.3878 Acc: 0.6570\n",
            "\n",
            "Epoch 448/499\n",
            "----------\n",
            "train Loss: 1.3469 Acc: 0.8440\n",
            "val Loss: 1.1009 Acc: 0.8560\n",
            "\n",
            "Epoch 449/499\n",
            "----------\n",
            "train Loss: 1.2156 Acc: 0.8565\n",
            "val Loss: 3.4894 Acc: 0.7503\n",
            "\n",
            "Epoch 450/499\n",
            "----------\n",
            "train Loss: 1.9418 Acc: 0.8037\n",
            "val Loss: 2.4904 Acc: 0.7888\n",
            "\n",
            "Epoch 451/499\n",
            "----------\n",
            "train Loss: 2.3064 Acc: 0.7930\n",
            "val Loss: 1.3439 Acc: 0.8520\n",
            "\n",
            "Epoch 452/499\n",
            "----------\n",
            "train Loss: 1.5045 Acc: 0.8357\n",
            "val Loss: 0.9071 Acc: 0.8817\n",
            "\n",
            "Epoch 453/499\n",
            "----------\n",
            "train Loss: 1.0124 Acc: 0.8647\n",
            "val Loss: 1.8775 Acc: 0.7602\n",
            "\n",
            "Epoch 454/499\n",
            "----------\n",
            "train Loss: 1.4258 Acc: 0.8361\n",
            "val Loss: 2.9390 Acc: 0.7337\n",
            "\n",
            "Epoch 455/499\n",
            "----------\n",
            "train Loss: 1.4474 Acc: 0.8350\n",
            "val Loss: 2.2804 Acc: 0.7462\n",
            "\n",
            "Epoch 456/499\n",
            "----------\n",
            "train Loss: 1.6765 Acc: 0.8205\n",
            "val Loss: 2.2223 Acc: 0.7539\n",
            "\n",
            "Epoch 457/499\n",
            "----------\n",
            "train Loss: 1.5030 Acc: 0.8348\n",
            "val Loss: 1.7716 Acc: 0.8032\n",
            "\n",
            "Epoch 458/499\n",
            "----------\n",
            "train Loss: 1.3463 Acc: 0.8410\n",
            "val Loss: 1.0330 Acc: 0.8502\n",
            "\n",
            "Epoch 459/499\n",
            "----------\n",
            "train Loss: 1.4552 Acc: 0.8367\n",
            "val Loss: 0.9832 Acc: 0.8711\n",
            "\n",
            "Epoch 460/499\n",
            "----------\n",
            "train Loss: 1.2643 Acc: 0.8464\n",
            "val Loss: 1.9360 Acc: 0.8054\n",
            "\n",
            "Epoch 461/499\n",
            "----------\n",
            "train Loss: 1.1406 Acc: 0.8494\n",
            "val Loss: 1.0243 Acc: 0.8634\n",
            "\n",
            "Epoch 462/499\n",
            "----------\n",
            "train Loss: 1.2213 Acc: 0.8457\n",
            "val Loss: 1.3511 Acc: 0.8340\n",
            "\n",
            "Epoch 463/499\n",
            "----------\n",
            "train Loss: 1.1181 Acc: 0.8517\n",
            "val Loss: 2.6732 Acc: 0.8369\n",
            "\n",
            "Epoch 464/499\n",
            "----------\n",
            "train Loss: 2.0402 Acc: 0.7977\n",
            "val Loss: 4.0252 Acc: 0.6757\n",
            "\n",
            "Epoch 465/499\n",
            "----------\n",
            "train Loss: 1.6160 Acc: 0.8156\n",
            "val Loss: 3.2763 Acc: 0.7470\n",
            "\n",
            "Epoch 466/499\n",
            "----------\n",
            "train Loss: 1.8184 Acc: 0.8111\n",
            "val Loss: 1.4713 Acc: 0.8076\n",
            "\n",
            "Epoch 467/499\n",
            "----------\n",
            "train Loss: 1.3980 Acc: 0.8340\n",
            "val Loss: 7.0964 Acc: 0.6555\n",
            "\n",
            "Epoch 468/499\n",
            "----------\n",
            "train Loss: 1.4961 Acc: 0.8363\n",
            "val Loss: 1.5711 Acc: 0.7903\n",
            "\n",
            "Epoch 469/499\n",
            "----------\n",
            "train Loss: 1.5508 Acc: 0.8261\n",
            "val Loss: 5.5912 Acc: 0.6698\n",
            "\n",
            "Epoch 470/499\n",
            "----------\n",
            "train Loss: 1.7599 Acc: 0.8095\n",
            "val Loss: 1.1614 Acc: 0.8340\n",
            "\n",
            "Epoch 471/499\n",
            "----------\n",
            "train Loss: 1.1433 Acc: 0.8508\n",
            "val Loss: 3.1822 Acc: 0.7455\n",
            "\n",
            "Epoch 472/499\n",
            "----------\n",
            "train Loss: 1.6408 Acc: 0.8229\n",
            "val Loss: 1.9341 Acc: 0.7694\n",
            "\n",
            "Epoch 473/499\n",
            "----------\n",
            "train Loss: 1.2121 Acc: 0.8429\n",
            "val Loss: 4.0249 Acc: 0.6504\n",
            "\n",
            "Epoch 474/499\n",
            "----------\n",
            "train Loss: 1.4796 Acc: 0.8269\n",
            "val Loss: 3.0687 Acc: 0.7253\n",
            "\n",
            "Epoch 475/499\n",
            "----------\n",
            "train Loss: 1.9044 Acc: 0.8083\n",
            "val Loss: 1.7994 Acc: 0.7881\n",
            "\n",
            "Epoch 476/499\n",
            "----------\n",
            "train Loss: 1.3407 Acc: 0.8388\n",
            "val Loss: 1.7268 Acc: 0.7804\n",
            "\n",
            "Epoch 477/499\n",
            "----------\n",
            "train Loss: 1.5069 Acc: 0.8259\n",
            "val Loss: 1.7089 Acc: 0.7943\n",
            "\n",
            "Epoch 478/499\n",
            "----------\n",
            "train Loss: 1.6370 Acc: 0.8100\n",
            "val Loss: 2.2850 Acc: 0.7800\n",
            "\n",
            "Epoch 479/499\n",
            "----------\n",
            "train Loss: 2.3262 Acc: 0.7706\n",
            "val Loss: 3.5250 Acc: 0.7389\n",
            "\n",
            "Epoch 480/499\n",
            "----------\n",
            "train Loss: 1.4965 Acc: 0.8234\n",
            "val Loss: 0.9612 Acc: 0.8520\n",
            "\n",
            "Epoch 481/499\n",
            "----------\n",
            "train Loss: 1.1981 Acc: 0.8405\n",
            "val Loss: 1.4873 Acc: 0.8314\n",
            "\n",
            "Epoch 482/499\n",
            "----------\n",
            "train Loss: 1.8459 Acc: 0.8161\n",
            "val Loss: 1.1852 Acc: 0.8380\n",
            "\n",
            "Epoch 483/499\n",
            "----------\n",
            "train Loss: 1.6772 Acc: 0.8096\n",
            "val Loss: 2.4599 Acc: 0.7606\n",
            "\n",
            "Epoch 484/499\n",
            "----------\n",
            "train Loss: 1.5440 Acc: 0.8172\n",
            "val Loss: 2.7592 Acc: 0.7282\n",
            "\n",
            "Epoch 485/499\n",
            "----------\n",
            "train Loss: 1.2696 Acc: 0.8418\n",
            "val Loss: 2.7896 Acc: 0.7517\n",
            "\n",
            "Epoch 486/499\n",
            "----------\n",
            "train Loss: 1.4644 Acc: 0.8282\n",
            "val Loss: 3.0537 Acc: 0.7286\n",
            "\n",
            "Epoch 487/499\n",
            "----------\n",
            "train Loss: 1.1478 Acc: 0.8454\n",
            "val Loss: 1.5630 Acc: 0.7863\n",
            "\n",
            "Epoch 488/499\n",
            "----------\n",
            "train Loss: 0.9891 Acc: 0.8610\n",
            "val Loss: 0.5902 Acc: 0.9023\n",
            "\n",
            "Epoch 489/499\n",
            "----------\n",
            "train Loss: 0.9283 Acc: 0.8637\n",
            "val Loss: 1.4275 Acc: 0.8072\n",
            "\n",
            "Epoch 490/499\n",
            "----------\n",
            "train Loss: 1.7522 Acc: 0.8128\n",
            "val Loss: 4.0380 Acc: 0.6214\n",
            "\n",
            "Epoch 491/499\n",
            "----------\n",
            "train Loss: 1.3762 Acc: 0.8254\n",
            "val Loss: 1.3422 Acc: 0.7881\n",
            "\n",
            "Epoch 492/499\n",
            "----------\n",
            "train Loss: 1.5328 Acc: 0.8138\n",
            "val Loss: 3.0093 Acc: 0.7007\n",
            "\n",
            "Epoch 493/499\n",
            "----------\n",
            "train Loss: 1.1182 Acc: 0.8432\n",
            "val Loss: 0.9025 Acc: 0.8604\n",
            "\n",
            "Epoch 494/499\n",
            "----------\n",
            "train Loss: 1.0577 Acc: 0.8534\n",
            "val Loss: 1.1489 Acc: 0.8263\n",
            "\n",
            "Epoch 495/499\n",
            "----------\n",
            "train Loss: 1.2537 Acc: 0.8306\n",
            "val Loss: 1.1538 Acc: 0.8061\n",
            "\n",
            "Epoch 496/499\n",
            "----------\n",
            "train Loss: 1.3057 Acc: 0.8291\n",
            "val Loss: 1.3375 Acc: 0.8223\n",
            "\n",
            "Epoch 497/499\n",
            "----------\n",
            "train Loss: 1.2188 Acc: 0.8344\n",
            "val Loss: 0.8473 Acc: 0.8693\n",
            "\n",
            "Epoch 498/499\n",
            "----------\n",
            "train Loss: 1.1945 Acc: 0.8358\n",
            "val Loss: 0.8536 Acc: 0.8674\n",
            "\n",
            "Epoch 499/499\n",
            "----------\n",
            "train Loss: 1.8897 Acc: 0.7901\n",
            "val Loss: 2.1099 Acc: 0.7437\n",
            "\n",
            "Training complete in 1m 32s\n",
            "Best val Acc: 0.902314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuJcj-hTp9g-",
        "outputId": "d1e2c761-f1b6-4729-8318-a4b76349311a"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(X_train, y_train)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSpJM-_yxuyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c95e7a-4276-4345-96f9-2f9aca9454dc"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_predict = random_forest.predict(X_test)\n",
        "accuracy_score(y_test, y_predict)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9250826294528094"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    }
  ]
}