{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Dry_Bean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWtTxQKV1_Gp",
        "outputId": "c2446378-10f7-45b8-a4c5-181863e310b3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlixUSTf1No9"
      },
      "source": [
        "import time\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WNQU04mT1NpD",
        "outputId": "2e0297c4-c31d-48f3-be45-a1201dc5657c"
      },
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/Dry_Bean/data/Dry_Bean_Dataset.xlsx\"\n",
        "dry_beans = pd.read_excel(DATA_PATH)\n",
        "dry_beans.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Perimeter</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>AspectRation</th>\n",
              "      <th>Eccentricity</th>\n",
              "      <th>ConvexArea</th>\n",
              "      <th>EquivDiameter</th>\n",
              "      <th>Extent</th>\n",
              "      <th>Solidity</th>\n",
              "      <th>roundness</th>\n",
              "      <th>Compactness</th>\n",
              "      <th>ShapeFactor1</th>\n",
              "      <th>ShapeFactor2</th>\n",
              "      <th>ShapeFactor3</th>\n",
              "      <th>ShapeFactor4</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28395</td>\n",
              "      <td>610.291</td>\n",
              "      <td>208.178117</td>\n",
              "      <td>173.888747</td>\n",
              "      <td>1.197191</td>\n",
              "      <td>0.549812</td>\n",
              "      <td>28715</td>\n",
              "      <td>190.141097</td>\n",
              "      <td>0.763923</td>\n",
              "      <td>0.988856</td>\n",
              "      <td>0.958027</td>\n",
              "      <td>0.913358</td>\n",
              "      <td>0.007332</td>\n",
              "      <td>0.003147</td>\n",
              "      <td>0.834222</td>\n",
              "      <td>0.998724</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28734</td>\n",
              "      <td>638.018</td>\n",
              "      <td>200.524796</td>\n",
              "      <td>182.734419</td>\n",
              "      <td>1.097356</td>\n",
              "      <td>0.411785</td>\n",
              "      <td>29172</td>\n",
              "      <td>191.272750</td>\n",
              "      <td>0.783968</td>\n",
              "      <td>0.984986</td>\n",
              "      <td>0.887034</td>\n",
              "      <td>0.953861</td>\n",
              "      <td>0.006979</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>0.909851</td>\n",
              "      <td>0.998430</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29380</td>\n",
              "      <td>624.110</td>\n",
              "      <td>212.826130</td>\n",
              "      <td>175.931143</td>\n",
              "      <td>1.209713</td>\n",
              "      <td>0.562727</td>\n",
              "      <td>29690</td>\n",
              "      <td>193.410904</td>\n",
              "      <td>0.778113</td>\n",
              "      <td>0.989559</td>\n",
              "      <td>0.947849</td>\n",
              "      <td>0.908774</td>\n",
              "      <td>0.007244</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>0.825871</td>\n",
              "      <td>0.999066</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30008</td>\n",
              "      <td>645.884</td>\n",
              "      <td>210.557999</td>\n",
              "      <td>182.516516</td>\n",
              "      <td>1.153638</td>\n",
              "      <td>0.498616</td>\n",
              "      <td>30724</td>\n",
              "      <td>195.467062</td>\n",
              "      <td>0.782681</td>\n",
              "      <td>0.976696</td>\n",
              "      <td>0.903936</td>\n",
              "      <td>0.928329</td>\n",
              "      <td>0.007017</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.861794</td>\n",
              "      <td>0.994199</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30140</td>\n",
              "      <td>620.134</td>\n",
              "      <td>201.847882</td>\n",
              "      <td>190.279279</td>\n",
              "      <td>1.060798</td>\n",
              "      <td>0.333680</td>\n",
              "      <td>30417</td>\n",
              "      <td>195.896503</td>\n",
              "      <td>0.773098</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>0.984877</td>\n",
              "      <td>0.970516</td>\n",
              "      <td>0.006697</td>\n",
              "      <td>0.003665</td>\n",
              "      <td>0.941900</td>\n",
              "      <td>0.999166</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Area  Perimeter  MajorAxisLength  ...  ShapeFactor3  ShapeFactor4  Class\n",
              "0  28395    610.291       208.178117  ...      0.834222      0.998724  SEKER\n",
              "1  28734    638.018       200.524796  ...      0.909851      0.998430  SEKER\n",
              "2  29380    624.110       212.826130  ...      0.825871      0.999066  SEKER\n",
              "3  30008    645.884       210.557999  ...      0.861794      0.994199  SEKER\n",
              "4  30140    620.134       201.847882  ...      0.941900      0.999166  SEKER\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qru6CveK3TKq"
      },
      "source": [
        "class DryBeanDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = pd.read_excel(path)\n",
        "        self.X = torch.Tensor(np.array(data.iloc[:, :-1], dtype=np.float32))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.y = torch.Tensor(self.label_encoder.fit_transform(data.iloc[:, -1]))\n",
        "        self.y_label_classes = self.label_encoder.classes_\n",
        "        self.n_samples = data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return self.X[item], self.y[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "    def inverse_encoder(self):\n",
        "        return self.y_label_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofbrAc0bEQBG"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(16,200)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(200, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = self.relu1(output)\n",
        "        output = self.fc2(output)\n",
        "        return output"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrahFDUj5_53"
      },
      "source": [
        "dry_bean = DryBeanDataset(DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKwuXtAkNN3"
      },
      "source": [
        "X, y = dry_bean[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of1FtD4f6_sC"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofT0a8JNtbf-",
        "outputId": "6032e0b1-055f-4a27-9978-c2dfda0d6606"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNF2Iu8Atgz3",
        "outputId": "bf326f83-b6aa-4fdb-cb32-733f72edae99"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7OUAwWWEEy_"
      },
      "source": [
        "train_data = TensorDataset(X_train, y_train.type(torch.LongTensor))\n",
        "test_data = TensorDataset(X_test, y_test.type(torch.LongTensor))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vylABOby1Utl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AZjfv8Dr1Lc"
      },
      "source": [
        "batch_size = 150\n",
        "learning_rate = 0.001\n",
        "num_epoch = 300"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb6MOgDGn-p5"
      },
      "source": [
        "network = MLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = torch.optim.Adam(network.parameters(), lr=learning_rate)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0wxMAlmoghA"
      },
      "source": [
        "data = {\"train\": train_data, \"val\": test_data}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(data[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(data[x]) for x in ['train', 'val']}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmhZk5f937YD"
      },
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rr_sg6NoIzv",
        "outputId": "a92fca8a-b463-499d-d5ff-3d703a58771e"
      },
      "source": [
        "model_ft = train_model(network, criterion, optimizer_ft, num_epochs=num_epoch)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/299\n",
            "----------\n",
            "train Loss: 889.0454 Acc: 0.1371\n",
            "val Loss: 606.4623 Acc: 0.1913\n",
            "\n",
            "Epoch 1/299\n",
            "----------\n",
            "train Loss: 438.8286 Acc: 0.1593\n",
            "val Loss: 380.1557 Acc: 0.2560\n",
            "\n",
            "Epoch 2/299\n",
            "----------\n",
            "train Loss: 295.7517 Acc: 0.1616\n",
            "val Loss: 366.0456 Acc: 0.1550\n",
            "\n",
            "Epoch 3/299\n",
            "----------\n",
            "train Loss: 448.3113 Acc: 0.1587\n",
            "val Loss: 594.2124 Acc: 0.3551\n",
            "\n",
            "Epoch 4/299\n",
            "----------\n",
            "train Loss: 340.6316 Acc: 0.1712\n",
            "val Loss: 355.8862 Acc: 0.2578\n",
            "\n",
            "Epoch 5/299\n",
            "----------\n",
            "train Loss: 325.1796 Acc: 0.1744\n",
            "val Loss: 464.3339 Acc: 0.1913\n",
            "\n",
            "Epoch 6/299\n",
            "----------\n",
            "train Loss: 274.1700 Acc: 0.1830\n",
            "val Loss: 281.8419 Acc: 0.0404\n",
            "\n",
            "Epoch 7/299\n",
            "----------\n",
            "train Loss: 281.3843 Acc: 0.1706\n",
            "val Loss: 216.0575 Acc: 0.3739\n",
            "\n",
            "Epoch 8/299\n",
            "----------\n",
            "train Loss: 274.6108 Acc: 0.2139\n",
            "val Loss: 325.9490 Acc: 0.1194\n",
            "\n",
            "Epoch 9/299\n",
            "----------\n",
            "train Loss: 213.8609 Acc: 0.2177\n",
            "val Loss: 227.7886 Acc: 0.2927\n",
            "\n",
            "Epoch 10/299\n",
            "----------\n",
            "train Loss: 218.1306 Acc: 0.2419\n",
            "val Loss: 100.9296 Acc: 0.3272\n",
            "\n",
            "Epoch 11/299\n",
            "----------\n",
            "train Loss: 170.4351 Acc: 0.2344\n",
            "val Loss: 91.3923 Acc: 0.1748\n",
            "\n",
            "Epoch 12/299\n",
            "----------\n",
            "train Loss: 144.8615 Acc: 0.2470\n",
            "val Loss: 149.8407 Acc: 0.0988\n",
            "\n",
            "Epoch 13/299\n",
            "----------\n",
            "train Loss: 206.8894 Acc: 0.2098\n",
            "val Loss: 258.3832 Acc: 0.2284\n",
            "\n",
            "Epoch 14/299\n",
            "----------\n",
            "train Loss: 158.2126 Acc: 0.2468\n",
            "val Loss: 181.6185 Acc: 0.2005\n",
            "\n",
            "Epoch 15/299\n",
            "----------\n",
            "train Loss: 177.1067 Acc: 0.2618\n",
            "val Loss: 279.0396 Acc: 0.1550\n",
            "\n",
            "Epoch 16/299\n",
            "----------\n",
            "train Loss: 183.8759 Acc: 0.2519\n",
            "val Loss: 212.0277 Acc: 0.1913\n",
            "\n",
            "Epoch 17/299\n",
            "----------\n",
            "train Loss: 196.9316 Acc: 0.2337\n",
            "val Loss: 61.7999 Acc: 0.1788\n",
            "\n",
            "Epoch 18/299\n",
            "----------\n",
            "train Loss: 107.1836 Acc: 0.2811\n",
            "val Loss: 99.8549 Acc: 0.1799\n",
            "\n",
            "Epoch 19/299\n",
            "----------\n",
            "train Loss: 143.3651 Acc: 0.2729\n",
            "val Loss: 92.1930 Acc: 0.2585\n",
            "\n",
            "Epoch 20/299\n",
            "----------\n",
            "train Loss: 146.1254 Acc: 0.2751\n",
            "val Loss: 127.9562 Acc: 0.3537\n",
            "\n",
            "Epoch 21/299\n",
            "----------\n",
            "train Loss: 132.7183 Acc: 0.2885\n",
            "val Loss: 100.6415 Acc: 0.1744\n",
            "\n",
            "Epoch 22/299\n",
            "----------\n",
            "train Loss: 168.1421 Acc: 0.2436\n",
            "val Loss: 133.1244 Acc: 0.3643\n",
            "\n",
            "Epoch 23/299\n",
            "----------\n",
            "train Loss: 126.4214 Acc: 0.2936\n",
            "val Loss: 121.3772 Acc: 0.2075\n",
            "\n",
            "Epoch 24/299\n",
            "----------\n",
            "train Loss: 118.7013 Acc: 0.2874\n",
            "val Loss: 236.9734 Acc: 0.2699\n",
            "\n",
            "Epoch 25/299\n",
            "----------\n",
            "train Loss: 110.6591 Acc: 0.2939\n",
            "val Loss: 68.8559 Acc: 0.1939\n",
            "\n",
            "Epoch 26/299\n",
            "----------\n",
            "train Loss: 122.5248 Acc: 0.3095\n",
            "val Loss: 75.3507 Acc: 0.1913\n",
            "\n",
            "Epoch 27/299\n",
            "----------\n",
            "train Loss: 94.6038 Acc: 0.3078\n",
            "val Loss: 108.4664 Acc: 0.1921\n",
            "\n",
            "Epoch 28/299\n",
            "----------\n",
            "train Loss: 77.1739 Acc: 0.3759\n",
            "val Loss: 125.8452 Acc: 0.4040\n",
            "\n",
            "Epoch 29/299\n",
            "----------\n",
            "train Loss: 110.6980 Acc: 0.3227\n",
            "val Loss: 72.4815 Acc: 0.3386\n",
            "\n",
            "Epoch 30/299\n",
            "----------\n",
            "train Loss: 61.4309 Acc: 0.3747\n",
            "val Loss: 100.9208 Acc: 0.1869\n",
            "\n",
            "Epoch 31/299\n",
            "----------\n",
            "train Loss: 128.7084 Acc: 0.2978\n",
            "val Loss: 86.7542 Acc: 0.3426\n",
            "\n",
            "Epoch 32/299\n",
            "----------\n",
            "train Loss: 90.0205 Acc: 0.3343\n",
            "val Loss: 57.8640 Acc: 0.4124\n",
            "\n",
            "Epoch 33/299\n",
            "----------\n",
            "train Loss: 132.0561 Acc: 0.3051\n",
            "val Loss: 84.9526 Acc: 0.3687\n",
            "\n",
            "Epoch 34/299\n",
            "----------\n",
            "train Loss: 78.9944 Acc: 0.3613\n",
            "val Loss: 96.5907 Acc: 0.3922\n",
            "\n",
            "Epoch 35/299\n",
            "----------\n",
            "train Loss: 83.3299 Acc: 0.3476\n",
            "val Loss: 119.6976 Acc: 0.3878\n",
            "\n",
            "Epoch 36/299\n",
            "----------\n",
            "train Loss: 76.9603 Acc: 0.3659\n",
            "val Loss: 54.7635 Acc: 0.3559\n",
            "\n",
            "Epoch 37/299\n",
            "----------\n",
            "train Loss: 83.7600 Acc: 0.3880\n",
            "val Loss: 147.7410 Acc: 0.3232\n",
            "\n",
            "Epoch 38/299\n",
            "----------\n",
            "train Loss: 75.9867 Acc: 0.3706\n",
            "val Loss: 40.1952 Acc: 0.5244\n",
            "\n",
            "Epoch 39/299\n",
            "----------\n",
            "train Loss: 93.5577 Acc: 0.3502\n",
            "val Loss: 27.2893 Acc: 0.4304\n",
            "\n",
            "Epoch 40/299\n",
            "----------\n",
            "train Loss: 58.7306 Acc: 0.4065\n",
            "val Loss: 70.3233 Acc: 0.3845\n",
            "\n",
            "Epoch 41/299\n",
            "----------\n",
            "train Loss: 83.8922 Acc: 0.3907\n",
            "val Loss: 79.6883 Acc: 0.4466\n",
            "\n",
            "Epoch 42/299\n",
            "----------\n",
            "train Loss: 73.4712 Acc: 0.3748\n",
            "val Loss: 46.7662 Acc: 0.5083\n",
            "\n",
            "Epoch 43/299\n",
            "----------\n",
            "train Loss: 52.2096 Acc: 0.4352\n",
            "val Loss: 28.5231 Acc: 0.5578\n",
            "\n",
            "Epoch 44/299\n",
            "----------\n",
            "train Loss: 54.5679 Acc: 0.4514\n",
            "val Loss: 61.1794 Acc: 0.3008\n",
            "\n",
            "Epoch 45/299\n",
            "----------\n",
            "train Loss: 70.5382 Acc: 0.4144\n",
            "val Loss: 75.8152 Acc: 0.4150\n",
            "\n",
            "Epoch 46/299\n",
            "----------\n",
            "train Loss: 65.1952 Acc: 0.4221\n",
            "val Loss: 132.1642 Acc: 0.2479\n",
            "\n",
            "Epoch 47/299\n",
            "----------\n",
            "train Loss: 64.3164 Acc: 0.4228\n",
            "val Loss: 22.4774 Acc: 0.5207\n",
            "\n",
            "Epoch 48/299\n",
            "----------\n",
            "train Loss: 57.9560 Acc: 0.4735\n",
            "val Loss: 24.2684 Acc: 0.4679\n",
            "\n",
            "Epoch 49/299\n",
            "----------\n",
            "train Loss: 42.7619 Acc: 0.4610\n",
            "val Loss: 26.5066 Acc: 0.3250\n",
            "\n",
            "Epoch 50/299\n",
            "----------\n",
            "train Loss: 46.6391 Acc: 0.4406\n",
            "val Loss: 18.9319 Acc: 0.4550\n",
            "\n",
            "Epoch 51/299\n",
            "----------\n",
            "train Loss: 57.5781 Acc: 0.4428\n",
            "val Loss: 35.6764 Acc: 0.5560\n",
            "\n",
            "Epoch 52/299\n",
            "----------\n",
            "train Loss: 98.2893 Acc: 0.3564\n",
            "val Loss: 43.6891 Acc: 0.5321\n",
            "\n",
            "Epoch 53/299\n",
            "----------\n",
            "train Loss: 35.8072 Acc: 0.4882\n",
            "val Loss: 21.7515 Acc: 0.5777\n",
            "\n",
            "Epoch 54/299\n",
            "----------\n",
            "train Loss: 32.4857 Acc: 0.5141\n",
            "val Loss: 24.0484 Acc: 0.5483\n",
            "\n",
            "Epoch 55/299\n",
            "----------\n",
            "train Loss: 41.9418 Acc: 0.4913\n",
            "val Loss: 35.8203 Acc: 0.5163\n",
            "\n",
            "Epoch 56/299\n",
            "----------\n",
            "train Loss: 35.5586 Acc: 0.4924\n",
            "val Loss: 57.6021 Acc: 0.5365\n",
            "\n",
            "Epoch 57/299\n",
            "----------\n",
            "train Loss: 51.6614 Acc: 0.4631\n",
            "val Loss: 66.1807 Acc: 0.4179\n",
            "\n",
            "Epoch 58/299\n",
            "----------\n",
            "train Loss: 54.4073 Acc: 0.4479\n",
            "val Loss: 71.8203 Acc: 0.5512\n",
            "\n",
            "Epoch 59/299\n",
            "----------\n",
            "train Loss: 55.9417 Acc: 0.4626\n",
            "val Loss: 87.8919 Acc: 0.4502\n",
            "\n",
            "Epoch 60/299\n",
            "----------\n",
            "train Loss: 57.6898 Acc: 0.4405\n",
            "val Loss: 81.6394 Acc: 0.4381\n",
            "\n",
            "Epoch 61/299\n",
            "----------\n",
            "train Loss: 55.2192 Acc: 0.4608\n",
            "val Loss: 93.8337 Acc: 0.4811\n",
            "\n",
            "Epoch 62/299\n",
            "----------\n",
            "train Loss: 66.5036 Acc: 0.4333\n",
            "val Loss: 104.1723 Acc: 0.5321\n",
            "\n",
            "Epoch 63/299\n",
            "----------\n",
            "train Loss: 54.6357 Acc: 0.4617\n",
            "val Loss: 37.2338 Acc: 0.6357\n",
            "\n",
            "Epoch 64/299\n",
            "----------\n",
            "train Loss: 41.2851 Acc: 0.5073\n",
            "val Loss: 26.9569 Acc: 0.4216\n",
            "\n",
            "Epoch 65/299\n",
            "----------\n",
            "train Loss: 32.2279 Acc: 0.5118\n",
            "val Loss: 17.4272 Acc: 0.6658\n",
            "\n",
            "Epoch 66/299\n",
            "----------\n",
            "train Loss: 41.5914 Acc: 0.5029\n",
            "val Loss: 52.8309 Acc: 0.5674\n",
            "\n",
            "Epoch 67/299\n",
            "----------\n",
            "train Loss: 30.6743 Acc: 0.5297\n",
            "val Loss: 45.4496 Acc: 0.4315\n",
            "\n",
            "Epoch 68/299\n",
            "----------\n",
            "train Loss: 43.9752 Acc: 0.4992\n",
            "val Loss: 23.4552 Acc: 0.6331\n",
            "\n",
            "Epoch 69/299\n",
            "----------\n",
            "train Loss: 85.3509 Acc: 0.4309\n",
            "val Loss: 80.1694 Acc: 0.3801\n",
            "\n",
            "Epoch 70/299\n",
            "----------\n",
            "train Loss: 45.9016 Acc: 0.4998\n",
            "val Loss: 11.4684 Acc: 0.6552\n",
            "\n",
            "Epoch 71/299\n",
            "----------\n",
            "train Loss: 29.4875 Acc: 0.5580\n",
            "val Loss: 29.8312 Acc: 0.5454\n",
            "\n",
            "Epoch 72/299\n",
            "----------\n",
            "train Loss: 42.0813 Acc: 0.4816\n",
            "val Loss: 34.3529 Acc: 0.6159\n",
            "\n",
            "Epoch 73/299\n",
            "----------\n",
            "train Loss: 32.0238 Acc: 0.5238\n",
            "val Loss: 25.5823 Acc: 0.5795\n",
            "\n",
            "Epoch 74/299\n",
            "----------\n",
            "train Loss: 44.0361 Acc: 0.5072\n",
            "val Loss: 60.7230 Acc: 0.6041\n",
            "\n",
            "Epoch 75/299\n",
            "----------\n",
            "train Loss: 29.8586 Acc: 0.5448\n",
            "val Loss: 13.8974 Acc: 0.6842\n",
            "\n",
            "Epoch 76/299\n",
            "----------\n",
            "train Loss: 30.9349 Acc: 0.5389\n",
            "val Loss: 42.2444 Acc: 0.5531\n",
            "\n",
            "Epoch 77/299\n",
            "----------\n",
            "train Loss: 46.9081 Acc: 0.5054\n",
            "val Loss: 40.6136 Acc: 0.4910\n",
            "\n",
            "Epoch 78/299\n",
            "----------\n",
            "train Loss: 50.0626 Acc: 0.4870\n",
            "val Loss: 18.4917 Acc: 0.6030\n",
            "\n",
            "Epoch 79/299\n",
            "----------\n",
            "train Loss: 41.1225 Acc: 0.5061\n",
            "val Loss: 8.6933 Acc: 0.6721\n",
            "\n",
            "Epoch 80/299\n",
            "----------\n",
            "train Loss: 26.8054 Acc: 0.5543\n",
            "val Loss: 35.4231 Acc: 0.5105\n",
            "\n",
            "Epoch 81/299\n",
            "----------\n",
            "train Loss: 40.8226 Acc: 0.5130\n",
            "val Loss: 68.0118 Acc: 0.5780\n",
            "\n",
            "Epoch 82/299\n",
            "----------\n",
            "train Loss: 35.5993 Acc: 0.5276\n",
            "val Loss: 8.4129 Acc: 0.6449\n",
            "\n",
            "Epoch 83/299\n",
            "----------\n",
            "train Loss: 22.6193 Acc: 0.5908\n",
            "val Loss: 19.5424 Acc: 0.6159\n",
            "\n",
            "Epoch 84/299\n",
            "----------\n",
            "train Loss: 30.2031 Acc: 0.5636\n",
            "val Loss: 19.1409 Acc: 0.5204\n",
            "\n",
            "Epoch 85/299\n",
            "----------\n",
            "train Loss: 21.8858 Acc: 0.5866\n",
            "val Loss: 32.6594 Acc: 0.5509\n",
            "\n",
            "Epoch 86/299\n",
            "----------\n",
            "train Loss: 33.5150 Acc: 0.5434\n",
            "val Loss: 59.5024 Acc: 0.5498\n",
            "\n",
            "Epoch 87/299\n",
            "----------\n",
            "train Loss: 30.7274 Acc: 0.5624\n",
            "val Loss: 20.8107 Acc: 0.5582\n",
            "\n",
            "Epoch 88/299\n",
            "----------\n",
            "train Loss: 35.6487 Acc: 0.5577\n",
            "val Loss: 85.6328 Acc: 0.4135\n",
            "\n",
            "Epoch 89/299\n",
            "----------\n",
            "train Loss: 42.1892 Acc: 0.5276\n",
            "val Loss: 28.8680 Acc: 0.6350\n",
            "\n",
            "Epoch 90/299\n",
            "----------\n",
            "train Loss: 34.5869 Acc: 0.5675\n",
            "val Loss: 52.8225 Acc: 0.4216\n",
            "\n",
            "Epoch 91/299\n",
            "----------\n",
            "train Loss: 28.8451 Acc: 0.6207\n",
            "val Loss: 30.0447 Acc: 0.6530\n",
            "\n",
            "Epoch 92/299\n",
            "----------\n",
            "train Loss: 34.2188 Acc: 0.5600\n",
            "val Loss: 56.6604 Acc: 0.4932\n",
            "\n",
            "Epoch 93/299\n",
            "----------\n",
            "train Loss: 27.6430 Acc: 0.5560\n",
            "val Loss: 74.4827 Acc: 0.3779\n",
            "\n",
            "Epoch 94/299\n",
            "----------\n",
            "train Loss: 36.7780 Acc: 0.5326\n",
            "val Loss: 21.0345 Acc: 0.5512\n",
            "\n",
            "Epoch 95/299\n",
            "----------\n",
            "train Loss: 21.9416 Acc: 0.6071\n",
            "val Loss: 17.5785 Acc: 0.6603\n",
            "\n",
            "Epoch 96/299\n",
            "----------\n",
            "train Loss: 23.6120 Acc: 0.6178\n",
            "val Loss: 22.8039 Acc: 0.5982\n",
            "\n",
            "Epoch 97/299\n",
            "----------\n",
            "train Loss: 41.7086 Acc: 0.5316\n",
            "val Loss: 41.6491 Acc: 0.6015\n",
            "\n",
            "Epoch 98/299\n",
            "----------\n",
            "train Loss: 21.4721 Acc: 0.6211\n",
            "val Loss: 29.1547 Acc: 0.5564\n",
            "\n",
            "Epoch 99/299\n",
            "----------\n",
            "train Loss: 33.9447 Acc: 0.5602\n",
            "val Loss: 31.1593 Acc: 0.6353\n",
            "\n",
            "Epoch 100/299\n",
            "----------\n",
            "train Loss: 36.8716 Acc: 0.5417\n",
            "val Loss: 37.8524 Acc: 0.3911\n",
            "\n",
            "Epoch 101/299\n",
            "----------\n",
            "train Loss: 24.7607 Acc: 0.5963\n",
            "val Loss: 37.5896 Acc: 0.4998\n",
            "\n",
            "Epoch 102/299\n",
            "----------\n",
            "train Loss: 27.3868 Acc: 0.5871\n",
            "val Loss: 34.8950 Acc: 0.5751\n",
            "\n",
            "Epoch 103/299\n",
            "----------\n",
            "train Loss: 23.8176 Acc: 0.5960\n",
            "val Loss: 17.0281 Acc: 0.6243\n",
            "\n",
            "Epoch 104/299\n",
            "----------\n",
            "train Loss: 25.1386 Acc: 0.6306\n",
            "val Loss: 10.7452 Acc: 0.6827\n",
            "\n",
            "Epoch 105/299\n",
            "----------\n",
            "train Loss: 17.3123 Acc: 0.6627\n",
            "val Loss: 15.7331 Acc: 0.6394\n",
            "\n",
            "Epoch 106/299\n",
            "----------\n",
            "train Loss: 35.9109 Acc: 0.5754\n",
            "val Loss: 34.0175 Acc: 0.5395\n",
            "\n",
            "Epoch 107/299\n",
            "----------\n",
            "train Loss: 26.5523 Acc: 0.5789\n",
            "val Loss: 19.2151 Acc: 0.7352\n",
            "\n",
            "Epoch 108/299\n",
            "----------\n",
            "train Loss: 35.6761 Acc: 0.5790\n",
            "val Loss: 53.7474 Acc: 0.5329\n",
            "\n",
            "Epoch 109/299\n",
            "----------\n",
            "train Loss: 33.5262 Acc: 0.5813\n",
            "val Loss: 27.3956 Acc: 0.5667\n",
            "\n",
            "Epoch 110/299\n",
            "----------\n",
            "train Loss: 21.6421 Acc: 0.6143\n",
            "val Loss: 15.0523 Acc: 0.7051\n",
            "\n",
            "Epoch 111/299\n",
            "----------\n",
            "train Loss: 18.7120 Acc: 0.6460\n",
            "val Loss: 41.2676 Acc: 0.7488\n",
            "\n",
            "Epoch 112/299\n",
            "----------\n",
            "train Loss: 33.0194 Acc: 0.5995\n",
            "val Loss: 34.0421 Acc: 0.5340\n",
            "\n",
            "Epoch 113/299\n",
            "----------\n",
            "train Loss: 31.3678 Acc: 0.5802\n",
            "val Loss: 33.6761 Acc: 0.5622\n",
            "\n",
            "Epoch 114/299\n",
            "----------\n",
            "train Loss: 17.7420 Acc: 0.6631\n",
            "val Loss: 35.5606 Acc: 0.4473\n",
            "\n",
            "Epoch 115/299\n",
            "----------\n",
            "train Loss: 16.8674 Acc: 0.6726\n",
            "val Loss: 17.3752 Acc: 0.6889\n",
            "\n",
            "Epoch 116/299\n",
            "----------\n",
            "train Loss: 34.9464 Acc: 0.5664\n",
            "val Loss: 15.2825 Acc: 0.6981\n",
            "\n",
            "Epoch 117/299\n",
            "----------\n",
            "train Loss: 33.2187 Acc: 0.5985\n",
            "val Loss: 30.7010 Acc: 0.7150\n",
            "\n",
            "Epoch 118/299\n",
            "----------\n",
            "train Loss: 16.4719 Acc: 0.6749\n",
            "val Loss: 15.5647 Acc: 0.6427\n",
            "\n",
            "Epoch 119/299\n",
            "----------\n",
            "train Loss: 16.0096 Acc: 0.6785\n",
            "val Loss: 27.1512 Acc: 0.7396\n",
            "\n",
            "Epoch 120/299\n",
            "----------\n",
            "train Loss: 25.0697 Acc: 0.6391\n",
            "val Loss: 21.4906 Acc: 0.6673\n",
            "\n",
            "Epoch 121/299\n",
            "----------\n",
            "train Loss: 28.9608 Acc: 0.6132\n",
            "val Loss: 31.7928 Acc: 0.5450\n",
            "\n",
            "Epoch 122/299\n",
            "----------\n",
            "train Loss: 18.2687 Acc: 0.6729\n",
            "val Loss: 15.4713 Acc: 0.6463\n",
            "\n",
            "Epoch 123/299\n",
            "----------\n",
            "train Loss: 14.3082 Acc: 0.6790\n",
            "val Loss: 28.0149 Acc: 0.7418\n",
            "\n",
            "Epoch 124/299\n",
            "----------\n",
            "train Loss: 20.9466 Acc: 0.6548\n",
            "val Loss: 5.6312 Acc: 0.7697\n",
            "\n",
            "Epoch 125/299\n",
            "----------\n",
            "train Loss: 23.3723 Acc: 0.6292\n",
            "val Loss: 56.9655 Acc: 0.4418\n",
            "\n",
            "Epoch 126/299\n",
            "----------\n",
            "train Loss: 30.0599 Acc: 0.6197\n",
            "val Loss: 12.3633 Acc: 0.7356\n",
            "\n",
            "Epoch 127/299\n",
            "----------\n",
            "train Loss: 16.9195 Acc: 0.7026\n",
            "val Loss: 7.8679 Acc: 0.7661\n",
            "\n",
            "Epoch 128/299\n",
            "----------\n",
            "train Loss: 17.4068 Acc: 0.6547\n",
            "val Loss: 18.9870 Acc: 0.5802\n",
            "\n",
            "Epoch 129/299\n",
            "----------\n",
            "train Loss: 20.0506 Acc: 0.6643\n",
            "val Loss: 12.9551 Acc: 0.6588\n",
            "\n",
            "Epoch 130/299\n",
            "----------\n",
            "train Loss: 19.6802 Acc: 0.6546\n",
            "val Loss: 22.9949 Acc: 0.5149\n",
            "\n",
            "Epoch 131/299\n",
            "----------\n",
            "train Loss: 14.8829 Acc: 0.6925\n",
            "val Loss: 42.2006 Acc: 0.6765\n",
            "\n",
            "Epoch 132/299\n",
            "----------\n",
            "train Loss: 25.2150 Acc: 0.6250\n",
            "val Loss: 14.4170 Acc: 0.7976\n",
            "\n",
            "Epoch 133/299\n",
            "----------\n",
            "train Loss: 13.1533 Acc: 0.6925\n",
            "val Loss: 14.6489 Acc: 0.7411\n",
            "\n",
            "Epoch 134/299\n",
            "----------\n",
            "train Loss: 31.8829 Acc: 0.6008\n",
            "val Loss: 12.9229 Acc: 0.6867\n",
            "\n",
            "Epoch 135/299\n",
            "----------\n",
            "train Loss: 16.5884 Acc: 0.6944\n",
            "val Loss: 33.3566 Acc: 0.6875\n",
            "\n",
            "Epoch 136/299\n",
            "----------\n",
            "train Loss: 14.8819 Acc: 0.7168\n",
            "val Loss: 19.4153 Acc: 0.7102\n",
            "\n",
            "Epoch 137/299\n",
            "----------\n",
            "train Loss: 19.1353 Acc: 0.6885\n",
            "val Loss: 10.7364 Acc: 0.6838\n",
            "\n",
            "Epoch 138/299\n",
            "----------\n",
            "train Loss: 25.6680 Acc: 0.6718\n",
            "val Loss: 19.9695 Acc: 0.5909\n",
            "\n",
            "Epoch 139/299\n",
            "----------\n",
            "train Loss: 20.8188 Acc: 0.6653\n",
            "val Loss: 9.3249 Acc: 0.7249\n",
            "\n",
            "Epoch 140/299\n",
            "----------\n",
            "train Loss: 13.6280 Acc: 0.7003\n",
            "val Loss: 7.2245 Acc: 0.7771\n",
            "\n",
            "Epoch 141/299\n",
            "----------\n",
            "train Loss: 23.9335 Acc: 0.6465\n",
            "val Loss: 12.4274 Acc: 0.6721\n",
            "\n",
            "Epoch 142/299\n",
            "----------\n",
            "train Loss: 13.2449 Acc: 0.7237\n",
            "val Loss: 9.7755 Acc: 0.7249\n",
            "\n",
            "Epoch 143/299\n",
            "----------\n",
            "train Loss: 14.5712 Acc: 0.6988\n",
            "val Loss: 36.0815 Acc: 0.5432\n",
            "\n",
            "Epoch 144/299\n",
            "----------\n",
            "train Loss: 21.1230 Acc: 0.6905\n",
            "val Loss: 16.5857 Acc: 0.8098\n",
            "\n",
            "Epoch 145/299\n",
            "----------\n",
            "train Loss: 11.7967 Acc: 0.7380\n",
            "val Loss: 4.7151 Acc: 0.8355\n",
            "\n",
            "Epoch 146/299\n",
            "----------\n",
            "train Loss: 15.5505 Acc: 0.7171\n",
            "val Loss: 38.1856 Acc: 0.5957\n",
            "\n",
            "Epoch 147/299\n",
            "----------\n",
            "train Loss: 23.1464 Acc: 0.6600\n",
            "val Loss: 10.9239 Acc: 0.7073\n",
            "\n",
            "Epoch 148/299\n",
            "----------\n",
            "train Loss: 24.6482 Acc: 0.6836\n",
            "val Loss: 19.3670 Acc: 0.7341\n",
            "\n",
            "Epoch 149/299\n",
            "----------\n",
            "train Loss: 10.8140 Acc: 0.7691\n",
            "val Loss: 22.4494 Acc: 0.6059\n",
            "\n",
            "Epoch 150/299\n",
            "----------\n",
            "train Loss: 30.5205 Acc: 0.6579\n",
            "val Loss: 85.8249 Acc: 0.3926\n",
            "\n",
            "Epoch 151/299\n",
            "----------\n",
            "train Loss: 28.4577 Acc: 0.6381\n",
            "val Loss: 28.7291 Acc: 0.5468\n",
            "\n",
            "Epoch 152/299\n",
            "----------\n",
            "train Loss: 27.7590 Acc: 0.6158\n",
            "val Loss: 8.5434 Acc: 0.7382\n",
            "\n",
            "Epoch 153/299\n",
            "----------\n",
            "train Loss: 13.7046 Acc: 0.7401\n",
            "val Loss: 14.3571 Acc: 0.7268\n",
            "\n",
            "Epoch 154/299\n",
            "----------\n",
            "train Loss: 26.6785 Acc: 0.6560\n",
            "val Loss: 34.2686 Acc: 0.5061\n",
            "\n",
            "Epoch 155/299\n",
            "----------\n",
            "train Loss: 19.5872 Acc: 0.6768\n",
            "val Loss: 4.8076 Acc: 0.8384\n",
            "\n",
            "Epoch 156/299\n",
            "----------\n",
            "train Loss: 8.8011 Acc: 0.7891\n",
            "val Loss: 20.7524 Acc: 0.6063\n",
            "\n",
            "Epoch 157/299\n",
            "----------\n",
            "train Loss: 9.6704 Acc: 0.7587\n",
            "val Loss: 18.4913 Acc: 0.6603\n",
            "\n",
            "Epoch 158/299\n",
            "----------\n",
            "train Loss: 19.6073 Acc: 0.6863\n",
            "val Loss: 34.7246 Acc: 0.6871\n",
            "\n",
            "Epoch 159/299\n",
            "----------\n",
            "train Loss: 16.2535 Acc: 0.7010\n",
            "val Loss: 17.2610 Acc: 0.7095\n",
            "\n",
            "Epoch 160/299\n",
            "----------\n",
            "train Loss: 11.4612 Acc: 0.7394\n",
            "val Loss: 26.8968 Acc: 0.6339\n",
            "\n",
            "Epoch 161/299\n",
            "----------\n",
            "train Loss: 15.8748 Acc: 0.7294\n",
            "val Loss: 5.1731 Acc: 0.8344\n",
            "\n",
            "Epoch 162/299\n",
            "----------\n",
            "train Loss: 7.9687 Acc: 0.7821\n",
            "val Loss: 9.0164 Acc: 0.7371\n",
            "\n",
            "Epoch 163/299\n",
            "----------\n",
            "train Loss: 16.8241 Acc: 0.6930\n",
            "val Loss: 14.8401 Acc: 0.6647\n",
            "\n",
            "Epoch 164/299\n",
            "----------\n",
            "train Loss: 12.4294 Acc: 0.7472\n",
            "val Loss: 6.3305 Acc: 0.8215\n",
            "\n",
            "Epoch 165/299\n",
            "----------\n",
            "train Loss: 18.7110 Acc: 0.6870\n",
            "val Loss: 19.1513 Acc: 0.6548\n",
            "\n",
            "Epoch 166/299\n",
            "----------\n",
            "train Loss: 17.3416 Acc: 0.7114\n",
            "val Loss: 12.6692 Acc: 0.7609\n",
            "\n",
            "Epoch 167/299\n",
            "----------\n",
            "train Loss: 12.2756 Acc: 0.7449\n",
            "val Loss: 16.6774 Acc: 0.6937\n",
            "\n",
            "Epoch 168/299\n",
            "----------\n",
            "train Loss: 9.5865 Acc: 0.7631\n",
            "val Loss: 9.9364 Acc: 0.7646\n",
            "\n",
            "Epoch 169/299\n",
            "----------\n",
            "train Loss: 19.0488 Acc: 0.7044\n",
            "val Loss: 11.3580 Acc: 0.7385\n",
            "\n",
            "Epoch 170/299\n",
            "----------\n",
            "train Loss: 23.1423 Acc: 0.6585\n",
            "val Loss: 40.4505 Acc: 0.6438\n",
            "\n",
            "Epoch 171/299\n",
            "----------\n",
            "train Loss: 23.5632 Acc: 0.6674\n",
            "val Loss: 21.9320 Acc: 0.7165\n",
            "\n",
            "Epoch 172/299\n",
            "----------\n",
            "train Loss: 10.1505 Acc: 0.7863\n",
            "val Loss: 4.7531 Acc: 0.8524\n",
            "\n",
            "Epoch 173/299\n",
            "----------\n",
            "train Loss: 9.4383 Acc: 0.7767\n",
            "val Loss: 17.3154 Acc: 0.7888\n",
            "\n",
            "Epoch 174/299\n",
            "----------\n",
            "train Loss: 16.2972 Acc: 0.7134\n",
            "val Loss: 8.4773 Acc: 0.7848\n",
            "\n",
            "Epoch 175/299\n",
            "----------\n",
            "train Loss: 9.8517 Acc: 0.7659\n",
            "val Loss: 7.9031 Acc: 0.7767\n",
            "\n",
            "Epoch 176/299\n",
            "----------\n",
            "train Loss: 9.8384 Acc: 0.7936\n",
            "val Loss: 3.3847 Acc: 0.8858\n",
            "\n",
            "Epoch 177/299\n",
            "----------\n",
            "train Loss: 30.7636 Acc: 0.6379\n",
            "val Loss: 18.1826 Acc: 0.7025\n",
            "\n",
            "Epoch 178/299\n",
            "----------\n",
            "train Loss: 13.8429 Acc: 0.7414\n",
            "val Loss: 14.2559 Acc: 0.7499\n",
            "\n",
            "Epoch 179/299\n",
            "----------\n",
            "train Loss: 10.7282 Acc: 0.7840\n",
            "val Loss: 68.7138 Acc: 0.5527\n",
            "\n",
            "Epoch 180/299\n",
            "----------\n",
            "train Loss: 12.9097 Acc: 0.7503\n",
            "val Loss: 5.4963 Acc: 0.8439\n",
            "\n",
            "Epoch 181/299\n",
            "----------\n",
            "train Loss: 20.3281 Acc: 0.6837\n",
            "val Loss: 8.5844 Acc: 0.7844\n",
            "\n",
            "Epoch 182/299\n",
            "----------\n",
            "train Loss: 14.0246 Acc: 0.7396\n",
            "val Loss: 6.7691 Acc: 0.8032\n",
            "\n",
            "Epoch 183/299\n",
            "----------\n",
            "train Loss: 9.4730 Acc: 0.7911\n",
            "val Loss: 25.7273 Acc: 0.6827\n",
            "\n",
            "Epoch 184/299\n",
            "----------\n",
            "train Loss: 17.9109 Acc: 0.7261\n",
            "val Loss: 39.0589 Acc: 0.6566\n",
            "\n",
            "Epoch 185/299\n",
            "----------\n",
            "train Loss: 14.7020 Acc: 0.7288\n",
            "val Loss: 15.7306 Acc: 0.6765\n",
            "\n",
            "Epoch 186/299\n",
            "----------\n",
            "train Loss: 11.0784 Acc: 0.7534\n",
            "val Loss: 6.3951 Acc: 0.7943\n",
            "\n",
            "Epoch 187/299\n",
            "----------\n",
            "train Loss: 9.7896 Acc: 0.7731\n",
            "val Loss: 21.3013 Acc: 0.7238\n",
            "\n",
            "Epoch 188/299\n",
            "----------\n",
            "train Loss: 9.1880 Acc: 0.7798\n",
            "val Loss: 9.8516 Acc: 0.7411\n",
            "\n",
            "Epoch 189/299\n",
            "----------\n",
            "train Loss: 6.7406 Acc: 0.8122\n",
            "val Loss: 11.4562 Acc: 0.7708\n",
            "\n",
            "Epoch 190/299\n",
            "----------\n",
            "train Loss: 21.8362 Acc: 0.7086\n",
            "val Loss: 13.2152 Acc: 0.7929\n",
            "\n",
            "Epoch 191/299\n",
            "----------\n",
            "train Loss: 10.5784 Acc: 0.7831\n",
            "val Loss: 10.0938 Acc: 0.7701\n",
            "\n",
            "Epoch 192/299\n",
            "----------\n",
            "train Loss: 19.6986 Acc: 0.6852\n",
            "val Loss: 12.5210 Acc: 0.7117\n",
            "\n",
            "Epoch 193/299\n",
            "----------\n",
            "train Loss: 21.4057 Acc: 0.6760\n",
            "val Loss: 24.1521 Acc: 0.6662\n",
            "\n",
            "Epoch 194/299\n",
            "----------\n",
            "train Loss: 12.3950 Acc: 0.7476\n",
            "val Loss: 9.2793 Acc: 0.7319\n",
            "\n",
            "Epoch 195/299\n",
            "----------\n",
            "train Loss: 11.3635 Acc: 0.7744\n",
            "val Loss: 6.0387 Acc: 0.8281\n",
            "\n",
            "Epoch 196/299\n",
            "----------\n",
            "train Loss: 11.3192 Acc: 0.7677\n",
            "val Loss: 4.6893 Acc: 0.8388\n",
            "\n",
            "Epoch 197/299\n",
            "----------\n",
            "train Loss: 8.3614 Acc: 0.8016\n",
            "val Loss: 5.3284 Acc: 0.8178\n",
            "\n",
            "Epoch 198/299\n",
            "----------\n",
            "train Loss: 9.9780 Acc: 0.7615\n",
            "val Loss: 14.8192 Acc: 0.7400\n",
            "\n",
            "Epoch 199/299\n",
            "----------\n",
            "train Loss: 15.8515 Acc: 0.7037\n",
            "val Loss: 10.6000 Acc: 0.7514\n",
            "\n",
            "Epoch 200/299\n",
            "----------\n",
            "train Loss: 8.0440 Acc: 0.7929\n",
            "val Loss: 5.7839 Acc: 0.8289\n",
            "\n",
            "Epoch 201/299\n",
            "----------\n",
            "train Loss: 10.7823 Acc: 0.7775\n",
            "val Loss: 8.6654 Acc: 0.7415\n",
            "\n",
            "Epoch 202/299\n",
            "----------\n",
            "train Loss: 12.5544 Acc: 0.7303\n",
            "val Loss: 11.1320 Acc: 0.7620\n",
            "\n",
            "Epoch 203/299\n",
            "----------\n",
            "train Loss: 13.8557 Acc: 0.7381\n",
            "val Loss: 10.6681 Acc: 0.8043\n",
            "\n",
            "Epoch 204/299\n",
            "----------\n",
            "train Loss: 16.0721 Acc: 0.7296\n",
            "val Loss: 18.9864 Acc: 0.6698\n",
            "\n",
            "Epoch 205/299\n",
            "----------\n",
            "train Loss: 16.6960 Acc: 0.7347\n",
            "val Loss: 17.9652 Acc: 0.7819\n",
            "\n",
            "Epoch 206/299\n",
            "----------\n",
            "train Loss: 19.8677 Acc: 0.7159\n",
            "val Loss: 5.2664 Acc: 0.8399\n",
            "\n",
            "Epoch 207/299\n",
            "----------\n",
            "train Loss: 6.8272 Acc: 0.8284\n",
            "val Loss: 4.9845 Acc: 0.8439\n",
            "\n",
            "Epoch 208/299\n",
            "----------\n",
            "train Loss: 10.6824 Acc: 0.7832\n",
            "val Loss: 15.1798 Acc: 0.7661\n",
            "\n",
            "Epoch 209/299\n",
            "----------\n",
            "train Loss: 11.8399 Acc: 0.7507\n",
            "val Loss: 3.7050 Acc: 0.8865\n",
            "\n",
            "Epoch 210/299\n",
            "----------\n",
            "train Loss: 8.5292 Acc: 0.8026\n",
            "val Loss: 7.1583 Acc: 0.7833\n",
            "\n",
            "Epoch 211/299\n",
            "----------\n",
            "train Loss: 6.0834 Acc: 0.8282\n",
            "val Loss: 5.8538 Acc: 0.8483\n",
            "\n",
            "Epoch 212/299\n",
            "----------\n",
            "train Loss: 12.6737 Acc: 0.7641\n",
            "val Loss: 9.5424 Acc: 0.7470\n",
            "\n",
            "Epoch 213/299\n",
            "----------\n",
            "train Loss: 9.7098 Acc: 0.7687\n",
            "val Loss: 21.6865 Acc: 0.6717\n",
            "\n",
            "Epoch 214/299\n",
            "----------\n",
            "train Loss: 11.8466 Acc: 0.7551\n",
            "val Loss: 10.6381 Acc: 0.7249\n",
            "\n",
            "Epoch 215/299\n",
            "----------\n",
            "train Loss: 9.5108 Acc: 0.7787\n",
            "val Loss: 8.9930 Acc: 0.8035\n",
            "\n",
            "Epoch 216/299\n",
            "----------\n",
            "train Loss: 9.3578 Acc: 0.7780\n",
            "val Loss: 11.6086 Acc: 0.7385\n",
            "\n",
            "Epoch 217/299\n",
            "----------\n",
            "train Loss: 9.7114 Acc: 0.7775\n",
            "val Loss: 38.9396 Acc: 0.6636\n",
            "\n",
            "Epoch 218/299\n",
            "----------\n",
            "train Loss: 13.1725 Acc: 0.7522\n",
            "val Loss: 4.6925 Acc: 0.8663\n",
            "\n",
            "Epoch 219/299\n",
            "----------\n",
            "train Loss: 13.7430 Acc: 0.7424\n",
            "val Loss: 5.8733 Acc: 0.8421\n",
            "\n",
            "Epoch 220/299\n",
            "----------\n",
            "train Loss: 11.8211 Acc: 0.7564\n",
            "val Loss: 5.7998 Acc: 0.8524\n",
            "\n",
            "Epoch 221/299\n",
            "----------\n",
            "train Loss: 10.7043 Acc: 0.7819\n",
            "val Loss: 22.4581 Acc: 0.8017\n",
            "\n",
            "Epoch 222/299\n",
            "----------\n",
            "train Loss: 8.4124 Acc: 0.7991\n",
            "val Loss: 6.1421 Acc: 0.8491\n",
            "\n",
            "Epoch 223/299\n",
            "----------\n",
            "train Loss: 9.2946 Acc: 0.7858\n",
            "val Loss: 7.6349 Acc: 0.7866\n",
            "\n",
            "Epoch 224/299\n",
            "----------\n",
            "train Loss: 7.0526 Acc: 0.8148\n",
            "val Loss: 3.7831 Acc: 0.8803\n",
            "\n",
            "Epoch 225/299\n",
            "----------\n",
            "train Loss: 7.0159 Acc: 0.8139\n",
            "val Loss: 11.6749 Acc: 0.8116\n",
            "\n",
            "Epoch 226/299\n",
            "----------\n",
            "train Loss: 11.5390 Acc: 0.7725\n",
            "val Loss: 29.2989 Acc: 0.7194\n",
            "\n",
            "Epoch 227/299\n",
            "----------\n",
            "train Loss: 12.6829 Acc: 0.7464\n",
            "val Loss: 5.9914 Acc: 0.8237\n",
            "\n",
            "Epoch 228/299\n",
            "----------\n",
            "train Loss: 8.4920 Acc: 0.8053\n",
            "val Loss: 6.8437 Acc: 0.8175\n",
            "\n",
            "Epoch 229/299\n",
            "----------\n",
            "train Loss: 12.0938 Acc: 0.7591\n",
            "val Loss: 19.0252 Acc: 0.7330\n",
            "\n",
            "Epoch 230/299\n",
            "----------\n",
            "train Loss: 18.8979 Acc: 0.7190\n",
            "val Loss: 8.4134 Acc: 0.8215\n",
            "\n",
            "Epoch 231/299\n",
            "----------\n",
            "train Loss: 8.3596 Acc: 0.7933\n",
            "val Loss: 7.8614 Acc: 0.7921\n",
            "\n",
            "Epoch 232/299\n",
            "----------\n",
            "train Loss: 11.7023 Acc: 0.7596\n",
            "val Loss: 4.4812 Acc: 0.8740\n",
            "\n",
            "Epoch 233/299\n",
            "----------\n",
            "train Loss: 7.8907 Acc: 0.8120\n",
            "val Loss: 3.9846 Acc: 0.8707\n",
            "\n",
            "Epoch 234/299\n",
            "----------\n",
            "train Loss: 8.3286 Acc: 0.8032\n",
            "val Loss: 7.4224 Acc: 0.8241\n",
            "\n",
            "Epoch 235/299\n",
            "----------\n",
            "train Loss: 12.3516 Acc: 0.7516\n",
            "val Loss: 9.9948 Acc: 0.7745\n",
            "\n",
            "Epoch 236/299\n",
            "----------\n",
            "train Loss: 10.7382 Acc: 0.7730\n",
            "val Loss: 11.5441 Acc: 0.7371\n",
            "\n",
            "Epoch 237/299\n",
            "----------\n",
            "train Loss: 8.3840 Acc: 0.8056\n",
            "val Loss: 6.0518 Acc: 0.8340\n",
            "\n",
            "Epoch 238/299\n",
            "----------\n",
            "train Loss: 5.9162 Acc: 0.8356\n",
            "val Loss: 3.4864 Acc: 0.8806\n",
            "\n",
            "Epoch 239/299\n",
            "----------\n",
            "train Loss: 7.0571 Acc: 0.8126\n",
            "val Loss: 3.6346 Acc: 0.8707\n",
            "\n",
            "Epoch 240/299\n",
            "----------\n",
            "train Loss: 9.6216 Acc: 0.7831\n",
            "val Loss: 8.8809 Acc: 0.7786\n",
            "\n",
            "Epoch 241/299\n",
            "----------\n",
            "train Loss: 7.9016 Acc: 0.8068\n",
            "val Loss: 20.7332 Acc: 0.7139\n",
            "\n",
            "Epoch 242/299\n",
            "----------\n",
            "train Loss: 7.0192 Acc: 0.8135\n",
            "val Loss: 4.9842 Acc: 0.8454\n",
            "\n",
            "Epoch 243/299\n",
            "----------\n",
            "train Loss: 7.1602 Acc: 0.8158\n",
            "val Loss: 11.6225 Acc: 0.7257\n",
            "\n",
            "Epoch 244/299\n",
            "----------\n",
            "train Loss: 12.1935 Acc: 0.7645\n",
            "val Loss: 15.8957 Acc: 0.6889\n",
            "\n",
            "Epoch 245/299\n",
            "----------\n",
            "train Loss: 15.8167 Acc: 0.7453\n",
            "val Loss: 22.4261 Acc: 0.6537\n",
            "\n",
            "Epoch 246/299\n",
            "----------\n",
            "train Loss: 18.0116 Acc: 0.7191\n",
            "val Loss: 41.1466 Acc: 0.5200\n",
            "\n",
            "Epoch 247/299\n",
            "----------\n",
            "train Loss: 12.4149 Acc: 0.7711\n",
            "val Loss: 7.5910 Acc: 0.8153\n",
            "\n",
            "Epoch 248/299\n",
            "----------\n",
            "train Loss: 8.0623 Acc: 0.8131\n",
            "val Loss: 6.8904 Acc: 0.8197\n",
            "\n",
            "Epoch 249/299\n",
            "----------\n",
            "train Loss: 6.2610 Acc: 0.8317\n",
            "val Loss: 5.1308 Acc: 0.8461\n",
            "\n",
            "Epoch 250/299\n",
            "----------\n",
            "train Loss: 5.2905 Acc: 0.8402\n",
            "val Loss: 6.2883 Acc: 0.8270\n",
            "\n",
            "Epoch 251/299\n",
            "----------\n",
            "train Loss: 9.0728 Acc: 0.7959\n",
            "val Loss: 5.8566 Acc: 0.8300\n",
            "\n",
            "Epoch 252/299\n",
            "----------\n",
            "train Loss: 16.1231 Acc: 0.7187\n",
            "val Loss: 13.7410 Acc: 0.7550\n",
            "\n",
            "Epoch 253/299\n",
            "----------\n",
            "train Loss: 9.4648 Acc: 0.7942\n",
            "val Loss: 5.7309 Acc: 0.8285\n",
            "\n",
            "Epoch 254/299\n",
            "----------\n",
            "train Loss: 7.8302 Acc: 0.8155\n",
            "val Loss: 4.9344 Acc: 0.8417\n",
            "\n",
            "Epoch 255/299\n",
            "----------\n",
            "train Loss: 5.9915 Acc: 0.8377\n",
            "val Loss: 9.0169 Acc: 0.7929\n",
            "\n",
            "Epoch 256/299\n",
            "----------\n",
            "train Loss: 6.6249 Acc: 0.8171\n",
            "val Loss: 3.4859 Acc: 0.8825\n",
            "\n",
            "Epoch 257/299\n",
            "----------\n",
            "train Loss: 6.9659 Acc: 0.8098\n",
            "val Loss: 6.0553 Acc: 0.8061\n",
            "\n",
            "Epoch 258/299\n",
            "----------\n",
            "train Loss: 9.4394 Acc: 0.7988\n",
            "val Loss: 4.2232 Acc: 0.8685\n",
            "\n",
            "Epoch 259/299\n",
            "----------\n",
            "train Loss: 9.4122 Acc: 0.7888\n",
            "val Loss: 3.4899 Acc: 0.8726\n",
            "\n",
            "Epoch 260/299\n",
            "----------\n",
            "train Loss: 4.7752 Acc: 0.8469\n",
            "val Loss: 6.7787 Acc: 0.8006\n",
            "\n",
            "Epoch 261/299\n",
            "----------\n",
            "train Loss: 18.2595 Acc: 0.7058\n",
            "val Loss: 85.2668 Acc: 0.4660\n",
            "\n",
            "Epoch 262/299\n",
            "----------\n",
            "train Loss: 17.2433 Acc: 0.7411\n",
            "val Loss: 6.4994 Acc: 0.8109\n",
            "\n",
            "Epoch 263/299\n",
            "----------\n",
            "train Loss: 7.0962 Acc: 0.8257\n",
            "val Loss: 7.1594 Acc: 0.7797\n",
            "\n",
            "Epoch 264/299\n",
            "----------\n",
            "train Loss: 14.7239 Acc: 0.7355\n",
            "val Loss: 52.2621 Acc: 0.7158\n",
            "\n",
            "Epoch 265/299\n",
            "----------\n",
            "train Loss: 18.9501 Acc: 0.7250\n",
            "val Loss: 13.4280 Acc: 0.7539\n",
            "\n",
            "Epoch 266/299\n",
            "----------\n",
            "train Loss: 6.8442 Acc: 0.8340\n",
            "val Loss: 5.2998 Acc: 0.8175\n",
            "\n",
            "Epoch 267/299\n",
            "----------\n",
            "train Loss: 7.9158 Acc: 0.8282\n",
            "val Loss: 5.5716 Acc: 0.8516\n",
            "\n",
            "Epoch 268/299\n",
            "----------\n",
            "train Loss: 9.3623 Acc: 0.7933\n",
            "val Loss: 25.0969 Acc: 0.7440\n",
            "\n",
            "Epoch 269/299\n",
            "----------\n",
            "train Loss: 7.9016 Acc: 0.8095\n",
            "val Loss: 7.0882 Acc: 0.8138\n",
            "\n",
            "Epoch 270/299\n",
            "----------\n",
            "train Loss: 10.7539 Acc: 0.7936\n",
            "val Loss: 4.1276 Acc: 0.8751\n",
            "\n",
            "Epoch 271/299\n",
            "----------\n",
            "train Loss: 7.5854 Acc: 0.8108\n",
            "val Loss: 9.5490 Acc: 0.8101\n",
            "\n",
            "Epoch 272/299\n",
            "----------\n",
            "train Loss: 5.5285 Acc: 0.8399\n",
            "val Loss: 5.8012 Acc: 0.7958\n",
            "\n",
            "Epoch 273/299\n",
            "----------\n",
            "train Loss: 5.0923 Acc: 0.8349\n",
            "val Loss: 4.7979 Acc: 0.8564\n",
            "\n",
            "Epoch 274/299\n",
            "----------\n",
            "train Loss: 4.7955 Acc: 0.8428\n",
            "val Loss: 11.9325 Acc: 0.7062\n",
            "\n",
            "Epoch 275/299\n",
            "----------\n",
            "train Loss: 15.5888 Acc: 0.7322\n",
            "val Loss: 7.1257 Acc: 0.8013\n",
            "\n",
            "Epoch 276/299\n",
            "----------\n",
            "train Loss: 4.8088 Acc: 0.8505\n",
            "val Loss: 3.4106 Acc: 0.8810\n",
            "\n",
            "Epoch 277/299\n",
            "----------\n",
            "train Loss: 5.5682 Acc: 0.8348\n",
            "val Loss: 12.5790 Acc: 0.7132\n",
            "\n",
            "Epoch 278/299\n",
            "----------\n",
            "train Loss: 12.4484 Acc: 0.7488\n",
            "val Loss: 3.9117 Acc: 0.8685\n",
            "\n",
            "Epoch 279/299\n",
            "----------\n",
            "train Loss: 10.2000 Acc: 0.7885\n",
            "val Loss: 8.3924 Acc: 0.8134\n",
            "\n",
            "Epoch 280/299\n",
            "----------\n",
            "train Loss: 8.4318 Acc: 0.7881\n",
            "val Loss: 3.5290 Acc: 0.8682\n",
            "\n",
            "Epoch 281/299\n",
            "----------\n",
            "train Loss: 10.1676 Acc: 0.7729\n",
            "val Loss: 6.8403 Acc: 0.8483\n",
            "\n",
            "Epoch 282/299\n",
            "----------\n",
            "train Loss: 5.4877 Acc: 0.8404\n",
            "val Loss: 4.8198 Acc: 0.8667\n",
            "\n",
            "Epoch 283/299\n",
            "----------\n",
            "train Loss: 8.2490 Acc: 0.7982\n",
            "val Loss: 4.8557 Acc: 0.8627\n",
            "\n",
            "Epoch 284/299\n",
            "----------\n",
            "train Loss: 10.5324 Acc: 0.7834\n",
            "val Loss: 10.1482 Acc: 0.7727\n",
            "\n",
            "Epoch 285/299\n",
            "----------\n",
            "train Loss: 5.8163 Acc: 0.8339\n",
            "val Loss: 4.8552 Acc: 0.8355\n",
            "\n",
            "Epoch 286/299\n",
            "----------\n",
            "train Loss: 7.4514 Acc: 0.8024\n",
            "val Loss: 5.7347 Acc: 0.8469\n",
            "\n",
            "Epoch 287/299\n",
            "----------\n",
            "train Loss: 5.9697 Acc: 0.8215\n",
            "val Loss: 4.6806 Acc: 0.8480\n",
            "\n",
            "Epoch 288/299\n",
            "----------\n",
            "train Loss: 7.2553 Acc: 0.8069\n",
            "val Loss: 16.4755 Acc: 0.7852\n",
            "\n",
            "Epoch 289/299\n",
            "----------\n",
            "train Loss: 7.3079 Acc: 0.8078\n",
            "val Loss: 6.4587 Acc: 0.8076\n",
            "\n",
            "Epoch 290/299\n",
            "----------\n",
            "train Loss: 5.9831 Acc: 0.8240\n",
            "val Loss: 3.7619 Acc: 0.8755\n",
            "\n",
            "Epoch 291/299\n",
            "----------\n",
            "train Loss: 5.7381 Acc: 0.8221\n",
            "val Loss: 12.1791 Acc: 0.7661\n",
            "\n",
            "Epoch 292/299\n",
            "----------\n",
            "train Loss: 7.0805 Acc: 0.7985\n",
            "val Loss: 7.3296 Acc: 0.7775\n",
            "\n",
            "Epoch 293/299\n",
            "----------\n",
            "train Loss: 9.0367 Acc: 0.7864\n",
            "val Loss: 16.4747 Acc: 0.7194\n",
            "\n",
            "Epoch 294/299\n",
            "----------\n",
            "train Loss: 9.0114 Acc: 0.8009\n",
            "val Loss: 20.0222 Acc: 0.6989\n",
            "\n",
            "Epoch 295/299\n",
            "----------\n",
            "train Loss: 9.6899 Acc: 0.7791\n",
            "val Loss: 8.1076 Acc: 0.7624\n",
            "\n",
            "Epoch 296/299\n",
            "----------\n",
            "train Loss: 5.1525 Acc: 0.8372\n",
            "val Loss: 3.7811 Acc: 0.8641\n",
            "\n",
            "Epoch 297/299\n",
            "----------\n",
            "train Loss: 7.0862 Acc: 0.8163\n",
            "val Loss: 11.4357 Acc: 0.7569\n",
            "\n",
            "Epoch 298/299\n",
            "----------\n",
            "train Loss: 9.8932 Acc: 0.7818\n",
            "val Loss: 5.4308 Acc: 0.8483\n",
            "\n",
            "Epoch 299/299\n",
            "----------\n",
            "train Loss: 6.0676 Acc: 0.8278\n",
            "val Loss: 13.6585 Acc: 0.7701\n",
            "\n",
            "Training complete in 0m 57s\n",
            "Best val Acc: 0.886522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuJcj-hTp9g-",
        "outputId": "810a534f-0671-4a1e-89f3-6828599b07e5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSpJM-_yxuyg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}